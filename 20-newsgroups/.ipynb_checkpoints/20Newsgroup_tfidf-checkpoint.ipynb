{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import re\n",
    "import gensim\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pyLDAvis.gensim\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "from gensim.test.utils import datapath\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing(doc):\n",
    "    \"\"\"\n",
    "    Convert a document into a list of lowercase tokens, ignoring tokens that are too short or too long.\n",
    "    \"\"\"\n",
    "    return gensim.utils.simple_preprocess(str(doc), deacc=True, min_len=4, max_len=15)\n",
    "\n",
    "\n",
    "def preprocessed_doc(doc):\n",
    "    \"\"\"\n",
    "    Preprocessing a single document\n",
    "    \"\"\"   \n",
    "    doc = re.sub(r'https?\\S*\\s?', '', doc)\n",
    "    doc = re.sub(r'\\s\\S*\\.edu\\S*\\s?', ' ', doc)\n",
    "    doc = re.sub(r'\\s\\S*\\.com\\S*\\s?', ' ', doc)\n",
    "    doc = re.sub(r'www\\.\\S*\\s?', '', doc)\n",
    "    doc = re.sub(r'\\S*@\\S*\\s?', '', doc)\n",
    "    doc = re.sub(r\"\\'\", \"\", doc)\n",
    "    doc = re.sub(r'\\s+', ' ', doc)  \n",
    "    doc = tokenizing(doc)\n",
    "    return doc\n",
    "    \n",
    "    \n",
    "def preprocessed_data(data):\n",
    "    \"\"\"\n",
    "    Preprocessing the entire data (list of documents)\n",
    "    \"\"\"\n",
    "    return [preprocessed_doc(doc) for doc in data]\n",
    "\n",
    "\n",
    "df = pandas.read_json('data/newsgroups.json')\n",
    "data = df.content.values.tolist()\n",
    "tokenized_data = preprocessed_data(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_ = ['that', 'from', 'this', 'have', 'with', 'subject', 'they', 'lines', 'organization',\n",
    "              'what', 'will', 'there', 'would', 'about', 'writes', 'your', 'article', 'some',\n",
    "              'which', 'were', 'more', 'people', 'like', 'dont', 'when', 'just', 'university',\n",
    "              'posting', 'their', 'other', 'know', 'only', 'host', 'them', 'nntp', 'than', 'been',\n",
    "              'think', 'also', 'does', 'time', 'then', 'good', 'these', 'well', 'should', 'could',\n",
    "              'because', 'even', 'very', 'into', 'first', 'many', 'those', 'make', 'much',\n",
    "              'most', 'system', 'such', 'distribution', 'right', 'where', 'world', 'want', 'here',\n",
    "              'reply', 'used', 'being', 'said', 'over', 'anyone', 'after', 'same', 'need', 'work',\n",
    "              'something', 'problem', 'please', 'really', 'computer', 'since', 'back', 'believe',\n",
    "              'still', 'going', 'years', 'file', 'information', 'year', 'windows', 'help', 'mail',\n",
    "              'using', 'state', 'find', 'take', 'question', 'last', 'point', 'thanks', 'space',\n",
    "              'before', 'must', 'never', 'things', 'while', 'better', 'government', 'cant', 'might',\n",
    "              'both', 'number', 'read', 'sure', 'another', 'case', 'without', 'program', 'down',\n",
    "              'through', 'made', 'data', 'drive', 'software', 'long', 'available', 'part', 'under',\n",
    "              'david', 'thing', 'doesnt', 'someone', 'look', 'power', 'thats', 'between', 'little',\n",
    "              'version', 'come', 'didnt', 'however', 'each', 'public', 'around', 'anything', 'fact',\n",
    "              'science', 'best', 'give', 'true', 'every', 'probably', 'again', 'name', 'john',\n",
    "              'course', 'least', 'line', 'against', 'tell', 'seems', 'group', 'different',\n",
    "              'systems', 'great', 'enough', 'high', 'research', 'news', 'list', 'hard', 'real',\n",
    "              'says', 'second', 'jesus', 'possible', 'either', 'life', 'actually', 'game',\n",
    "              'though', 'support', 'card', 'technology', 'post', 'center', 'called', 'free',\n",
    "              'rather', 'nothing', 'access', 'next', 'team', 'chip', 'window', 'mean',\n",
    "              'email', 'internet', 'problems', 'youre']\n",
    "\n",
    "with open('scripts/stopwords.txt', 'r') as fp:\n",
    "    for word in fp:\n",
    "        stopwords_.append(word.strip())\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(stopwords_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(doc):\n",
    "    \"\"\"\n",
    "    Removes stopwords from a document\n",
    "    \"\"\"\n",
    "    return [word for word in doc\n",
    "            if word not in stop_words]\n",
    "\n",
    "\n",
    "def make_bigrams(doc):\n",
    "    \"\"\"\n",
    "    Make bigrams of a document\n",
    "    \"\"\"\n",
    "    return bigram_mod[doc]\n",
    "\n",
    "\n",
    "def lemmatization(doc):\n",
    "    \"\"\"\n",
    "    Lemmatizes a document\n",
    "    \"\"\"\n",
    "    doc = nlp(\" \".join(doc))\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "\n",
    "def processed_doc(doc):\n",
    "    \"\"\"\n",
    "    Processing a document\n",
    "    \"\"\"\n",
    "    doc = remove_stopwords(doc)\n",
    "    doc = make_bigrams(doc)\n",
    "    doc = lemmatization(doc)\n",
    "    doc = remove_stopwords(doc)\n",
    "    return doc\n",
    "\n",
    "\n",
    "def processed_data(data):\n",
    "    \"\"\"\n",
    "    Return lemmatized data\n",
    "    \"\"\"\n",
    "    return [processed_doc(doc) for doc in data]\n",
    "\n",
    "\n",
    "bigram = gensim.models.Phrases(tokenized_data, min_count=5, threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "lemmatized_data = processed_data(tokenized_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_topics = 20\n",
    "lda_model = gensim.models.LdaModel.load(datapath('20newsgroups_tfidf'))\n",
    "id2word = gensim.corpora.Dictionary(lemmatized_data)\n",
    "corpus = corpus = [id2word.doc2bow(text) for text in lemmatized_data]\n",
    "\n",
    "tfidf = gensim.models.TfidfModel(corpus)\n",
    "corpus = tfidf[corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics():\n",
    "    \"\"\"\n",
    "    Prints top 30 words from each topic\n",
    "    \"\"\"\n",
    "    pprint(lda_model.show_topics(num_of_topics, 30))\n",
    "\n",
    "\n",
    "def get_doc_topic_distribution(index):\n",
    "    \"\"\"\n",
    "    Get the topic distribution for a given document\n",
    "    \"\"\"\n",
    "    doc = corpus[index]\n",
    "    return lda_model.get_document_topics(doc)\n",
    "\n",
    "\n",
    "def get_topic_term():\n",
    "    \"\"\"\n",
    "    Get topic-term matrix\n",
    "    \"\"\"\n",
    "    topic_term_matrix = lda_model.get_topics()\n",
    "    print(len(topic_term_matrix), 'x', len(topic_term_matrix[0]))\n",
    "    return topic_term_matrix\n",
    "\n",
    "\n",
    "def get_term_topics(word_id):\n",
    "    \"\"\"\n",
    "    Get the most relevant topics to the given word\n",
    "    \"\"\"\n",
    "    print('Word is: ', id2word[word_id])\n",
    "    relevant_topics = lda_model.get_term_topics(word_id)\n",
    "    relevant_topic_ids = [topic_id[0] for topic_id in relevant_topics]\n",
    "    for topic_id in relevant_topic_ids:\n",
    "        print(show_topic(topic_id))\n",
    "    return relevant_topics\n",
    "\n",
    "\n",
    "def unseen_doc_topic_distribution(new_doc: list):\n",
    "    \"\"\"\n",
    "    Get topic distribution of a unseen document\n",
    "    \"\"\"\n",
    "    new_doc = [id2word.doc2bow(new_doc)]\n",
    "    topics = lda_model[new_doc]\n",
    "    return sorted(topics[0][0], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "def doc_topic_matrix():\n",
    "    \"\"\"\n",
    "    Get document topic matrix\n",
    "    \"\"\"\n",
    "    matrix = np.zeros(shape=(len(corpus), num_of_topics))\n",
    "    for i in range(len(corpus)):\n",
    "        topic_dist = get_doc_topic_distribution(i)\n",
    "        for topic, prob in topic_dist:\n",
    "            matrix[i][topic] = prob\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def jensen_shannon(query, matrix):\n",
    "    \"\"\"\n",
    "    This function implements a Jensen-Shannon similarity\n",
    "    between the input query (an LDA topic distribution for a document)\n",
    "    and the entire corpus of topic distributions.\n",
    "    It returns an array of length M where M is the number of documents in the corpus\n",
    "    \"\"\"\n",
    "    p = query[None, :].T\n",
    "    q = matrix.T\n",
    "    m = 0.5 * (p + q)\n",
    "    # entropy calculated KL divergence\n",
    "    return np.sqrt(0.5 * (entropy(p, m) + entropy(q, m)))\n",
    "\n",
    "\n",
    "def get_document(index):\n",
    "    \"\"\"\n",
    "    index is list or num\n",
    "    \"\"\"\n",
    "    if isinstance(index, list):\n",
    "        docs = []\n",
    "        for i in index:\n",
    "            docs.append(data[i])\n",
    "        return docs\n",
    "    return data[index]\n",
    "\n",
    "\n",
    "def get_similar_docs(new_doc: list):\n",
    "    \"\"\"\n",
    "    This function implements the Jensen-Shannon distance above\n",
    "    and retruns the top k indices of the smallest jensen shannon distances\n",
    "    \"\"\"\n",
    "    topics = unseen_doc_topic_distribution(new_doc)\n",
    "    topic_dist = np.zeros(shape=(num_of_topics))\n",
    "    for topic, prob in topics:\n",
    "        topic_dist[topic] = prob\n",
    "    sims = jensen_shannon(topic_dist, doc_topic_matrix())\n",
    "    docs = list(sims.argsort()[:10])\n",
    "    return get_document(docs)\n",
    "\n",
    "\n",
    "def retrieval(doc):\n",
    "    doc = preprocessed_doc(doc)\n",
    "    doc = processed_doc(doc)\n",
    "    return get_similar_docs(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = '''\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Cubs mailing list\n",
      "From: andrew@dark.side.of.the.moon.uoknor.edu (Chihuahua Charlie)\n",
      "Distribution: usa\n",
      "Organization: OU - Academic User Services\n",
      "Nntp-Posting-Host: loopback.uoknor.edu\n",
      "News-Software: VAX/VMS VNEWS 1.41    Lines: 14\n",
      "Lines: 14\n",
      "\n",
      "\n",
      "\tIs there anyone out there running a Chicago National\n",
      "\tLeague Ballclub list?  If so, please send me information\n",
      "\ton it to...\n",
      "\t\t\tandrew@aardvark.ucs.uoknor.edu\n",
      "\n",
      "\tThanks!\n",
      "\n",
      "|\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/|\n",
      "|O|  _    |  Chihuahua Charlie              |  OU is not responsible   |O|\n",
      "|O| | |   |  Academic User Services         |  for anything anywhere,  |O|\n",
      "|O| ||||  |  The University of Oklahoma     |  except for that one     |O|\n",
      "|O|  |_|  |  andrew@aardvark.ucs.uoknor.edu |  incident where 200...   |O|\n",
      "|O|____________________________________________________________________|O|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "similar_doc1 = retrieval(doc1)\n",
    "print(similar_doc1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
