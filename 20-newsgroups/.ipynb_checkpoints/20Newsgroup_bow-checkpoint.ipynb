{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import re\n",
    "import gensim\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pyLDAvis.gensim\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "from gensim.test.utils import datapath\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing(doc):\n",
    "    \"\"\"\n",
    "    Convert a document into a list of lowercase tokens, ignoring tokens that are too short or too long.\n",
    "    \"\"\"\n",
    "    return gensim.utils.simple_preprocess(str(doc), deacc=True, min_len=4, max_len=15)\n",
    "\n",
    "\n",
    "def preprocessed_doc(doc):\n",
    "    \"\"\"\n",
    "    Preprocessing a single document\n",
    "    \"\"\"   \n",
    "    doc = re.sub(r'https?\\S*\\s?', '', doc)\n",
    "    doc = re.sub(r'\\s\\S*\\.edu\\S*\\s?', ' ', doc)\n",
    "    doc = re.sub(r'\\s\\S*\\.com\\S*\\s?', ' ', doc)\n",
    "    doc = re.sub(r'www\\.\\S*\\s?', '', doc)\n",
    "    doc = re.sub(r'\\S*@\\S*\\s?', '', doc)\n",
    "    doc = re.sub(r\"\\'\", \"\", doc)\n",
    "    doc = re.sub(r'\\s+', ' ', doc)  \n",
    "    doc = tokenizing(doc)\n",
    "    return doc\n",
    "    \n",
    "    \n",
    "def preprocessed_data(data):\n",
    "    \"\"\"\n",
    "    Preprocessing the entire data (list of documents)\n",
    "    \"\"\"\n",
    "    return [preprocessed_doc(doc) for doc in data]\n",
    "\n",
    "\n",
    "df = pandas.read_json('data/processed_iitdh_broadcast-3.json')\n",
    "data = df.content.values.tolist()\n",
    "tokenized_data = preprocessed_data(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_ = ['that', 'from', 'this', 'have', 'with', 'subject', 'they', 'lines', 'organization',\n",
    "              'what', 'will', 'there', 'would', 'about', 'writes', 'your', 'article', 'some',\n",
    "              'which', 'were', 'more', 'people', 'like', 'dont', 'when', 'just', 'university',\n",
    "              'posting', 'their', 'other', 'know', 'only', 'host', 'them', 'nntp', 'than', 'been',\n",
    "              'think', 'also', 'does', 'time', 'then', 'good', 'these', 'well', 'should', 'could',\n",
    "              'because', 'even', 'very', 'into', 'first', 'many', 'those', 'make', 'much',\n",
    "              'most', 'system', 'such', 'distribution', 'right', 'where', 'world', 'want', 'here',\n",
    "              'reply', 'used', 'being', 'said', 'over', 'anyone', 'after', 'same', 'need', 'work',\n",
    "              'something', 'problem', 'please', 'really', 'computer', 'since', 'back', 'believe',\n",
    "              'still', 'going', 'years', 'file', 'information', 'year', 'windows', 'help', 'mail',\n",
    "              'using', 'state', 'find', 'take', 'question', 'last', 'point', 'thanks', 'space',\n",
    "              'before', 'must', 'never', 'things', 'while', 'better', 'government', 'cant', 'might',\n",
    "              'both', 'number', 'read', 'sure', 'another', 'case', 'without', 'program', 'down',\n",
    "              'through', 'made', 'data', 'drive', 'software', 'long', 'available', 'part', 'under',\n",
    "              'david', 'thing', 'doesnt', 'someone', 'look', 'power', 'thats', 'between', 'little',\n",
    "              'version', 'come', 'didnt', 'however', 'each', 'public', 'around', 'anything', 'fact',\n",
    "              'science', 'best', 'give', 'true', 'every', 'probably', 'again', 'name', 'john',\n",
    "              'course', 'least', 'line', 'against', 'tell', 'seems', 'group', 'different',\n",
    "              'systems', 'great', 'enough', 'high', 'research', 'news', 'list', 'hard', 'real',\n",
    "              'says', 'second', 'jesus', 'possible', 'either', 'life', 'actually', 'game',\n",
    "              'though', 'support', 'card', 'technology', 'post', 'center', 'called', 'free',\n",
    "              'rather', 'nothing', 'access', 'next', 'team', 'chip', 'window', 'mean',\n",
    "              'email', 'internet', 'problems', 'youre']\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(stopwords_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(doc):\n",
    "    \"\"\"\n",
    "    Removes stopwords from a document\n",
    "    \"\"\"\n",
    "    return [word for word in doc\n",
    "            if word not in stop_words]\n",
    "\n",
    "\n",
    "def make_bigrams(doc):\n",
    "    \"\"\"\n",
    "    Make bigrams of a document\n",
    "    \"\"\"\n",
    "    return bigram_mod[doc]\n",
    "\n",
    "\n",
    "def lemmatization(doc):\n",
    "    \"\"\"\n",
    "    Lemmatizes a document\n",
    "    \"\"\"\n",
    "    doc = nlp(\" \".join(doc))\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "\n",
    "def processed_doc(doc):\n",
    "    \"\"\"\n",
    "    Processing a document\n",
    "    \"\"\"\n",
    "    doc = remove_stopwords(doc)\n",
    "    doc = make_bigrams(doc)\n",
    "    doc = lemmatization(doc)\n",
    "    doc = remove_stopwords(doc)\n",
    "    return doc\n",
    "\n",
    "\n",
    "def processed_data(data):\n",
    "    \"\"\"\n",
    "    Return lemmatized data\n",
    "    \"\"\"\n",
    "    return [processed_doc(doc) for doc in data]\n",
    "\n",
    "\n",
    "bigram = gensim.models.Phrases(tokenized_data, min_count=5, threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "lemmatized_data = processed_data(tokenized_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_topics = 20\n",
    "lda_model = gensim.models.LdaModel.load(datapath('20newsgroups_bow'))\n",
    "id2word = gensim.corpora.Dictionary(lemmatized_data)\n",
    "corpus = corpus = [id2word.doc2bow(text) for text in lemmatized_data]\n",
    "\n",
    "# tfidf = gensim.models.TfidfModel(corpus)\n",
    "# corpus = tfidf[corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics():\n",
    "    \"\"\"\n",
    "    Prints top 30 words from each topic\n",
    "    \"\"\"\n",
    "    pprint(lda_model.show_topics(num_of_topics, 30))\n",
    "\n",
    "\n",
    "def get_doc_topic_distribution(index):\n",
    "    \"\"\"\n",
    "    Get the topic distribution for a given document\n",
    "    \"\"\"\n",
    "    doc = corpus[index]\n",
    "    return lda_model.get_document_topics(doc)\n",
    "\n",
    "\n",
    "def get_topic_term():\n",
    "    \"\"\"\n",
    "    Get topic-term matrix\n",
    "    \"\"\"\n",
    "    topic_term_matrix = lda_model.get_topics()\n",
    "    print(len(topic_term_matrix), 'x', len(topic_term_matrix[0]))\n",
    "    return topic_term_matrix\n",
    "\n",
    "\n",
    "def get_term_topics(word_id):\n",
    "    \"\"\"\n",
    "    Get the most relevant topics to the given word\n",
    "    \"\"\"\n",
    "    print('Word is: ', id2word[word_id])\n",
    "    relevant_topics = lda_model.get_term_topics(word_id)\n",
    "    relevant_topic_ids = [topic_id[0] for topic_id in relevant_topics]\n",
    "    for topic_id in relevant_topic_ids:\n",
    "        print(show_topic(topic_id))\n",
    "    return relevant_topics\n",
    "\n",
    "\n",
    "def unseen_doc_topic_distribution(new_doc: list):\n",
    "    \"\"\"\n",
    "    Get topic distribution of a unseen document\n",
    "    \"\"\"\n",
    "    new_doc = [id2word.doc2bow(new_doc)]\n",
    "    topics = lda_model[new_doc]\n",
    "    return sorted(topics[0][0], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "def doc_topic_matrix():\n",
    "    \"\"\"\n",
    "    Get document topic matrix\n",
    "    \"\"\"\n",
    "    matrix = np.zeros(shape=(len(corpus), num_of_topics))\n",
    "    for i in range(len(corpus)):\n",
    "        topic_dist = get_doc_topic_distribution(i)\n",
    "        for topic, prob in topic_dist:\n",
    "            matrix[i][topic] = prob\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def jensen_shannon(query, matrix):\n",
    "    \"\"\"\n",
    "    This function implements a Jensen-Shannon similarity\n",
    "    between the input query (an LDA topic distribution for a document)\n",
    "    and the entire corpus of topic distributions.\n",
    "    It returns an array of length M where M is the number of documents in the corpus\n",
    "    \"\"\"\n",
    "    p = query[None, :].T\n",
    "    q = matrix.T\n",
    "    m = 0.5 * (p + q)\n",
    "    # entropy calculated KL divergence\n",
    "    return np.sqrt(0.5 * (entropy(p, m) + entropy(q, m)))\n",
    "\n",
    "\n",
    "def get_document(index):\n",
    "    \"\"\"\n",
    "    index is list or num\n",
    "    \"\"\"\n",
    "    if isinstance(index, list):\n",
    "        docs = []\n",
    "        for i in index:\n",
    "            docs.append(data[i])\n",
    "        return docs\n",
    "    return data[index]\n",
    "\n",
    "\n",
    "def get_similar_docs(new_doc: list):\n",
    "    \"\"\"\n",
    "    This function implements the Jensen-Shannon distance above\n",
    "    and retruns the top k indices of the smallest jensen shannon distances\n",
    "    \"\"\"\n",
    "    topics = unseen_doc_topic_distribution(new_doc)\n",
    "    topic_dist = np.zeros(shape=(num_of_topics))\n",
    "    for topic, prob in topics:\n",
    "        topic_dist[topic] = prob\n",
    "    sims = jensen_shannon(topic_dist, doc_topic_matrix())\n",
    "    docs = list(sims.argsort()[:10])\n",
    "    return get_document(docs)\n",
    "\n",
    "\n",
    "def retrieval(doc):\n",
    "    doc = preprocessed_doc(doc)\n",
    "    doc = processed_doc(doc)\n",
    "    return get_similar_docs(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = '''Dear Sir/Madam,\n",
    "\n",
    " \n",
    "\n",
    "Atal Bihari Vajpayee-Indian Institute of Information Technology and Management Gwalior (ABV-IIITM Gwalior) is seeking admission for a PhD programme in the field of Engineering Technology (CS & IT, EC), Management and Applied Sciences (Mathematics & Physics).\n",
    "\n",
    "Please find the attached programme brochure with specific programme details. We would like to request you to kindly circulate the brochure in your Institute so that the information may reach to the prospective research scholars.  \n",
    "\n",
    "Sincerely yours,\n",
    "\n",
    "Pankaj Gupta\n",
    "\n",
    "Joint Registrar (Academics)\n",
    "\n",
    " \n",
    "\n",
    "About the Institute:\n",
    "\n",
    "Atal Bihari Vajpayee-Indian Institute of Information Technology and Management Gwalior (ABV-IIITM Gwalior), is an apex Information Technology (IT) and Management Institute, established by the Government of India. ABV-IIITM Gwalior has been declared an Institute of National importance. The Institute strives to become a world-class Institution which endeavors to carve young minds through teaching and research and develop them as tomorrow's leaders. The Institute's mandate is to create Information Technology enabled Management solutions for nation building. The Institute offers various programmes at UG/PG and Doctoral level\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: DoMS, IITM conducts Algo Trading training program along with NSE\n",
      "Text: ---------- Forwarded message ----------\n",
      "From: Krishna Prasana P <pkp@iitm.ac.in>\n",
      "Date: Tue, Jul 10, 2018 at 3:54 PM\n",
      "Subject: DoMS, IITM conducts Algo Trading training program along with NSE\n",
      "To: \"director@iitdh.ac.in\" <director@iitdh.ac.in>\n",
      "\n",
      "\n",
      "Dear Sir/Madam\n",
      "\n",
      "Department of Management studies (DoMS) in association with National Stock\n",
      "Exchange (NSE Academy ) is organizing 2 – Day Training Program on\n",
      "ALGORITHMIC TRADING AND WINNING STRATEGIES on *17th and 18th *August, 2018.\n",
      "\n",
      "This program is open for students from IITs, NITs, and IIITs across various\n",
      "disciplines (B. Tech, Dual degree, M. Tech, MS, MBA).\n",
      "\n",
      "Algo trading is interdisciplinary in nature and calls for both trading\n",
      "skills and software proficiency. This program intends to prepare\n",
      "technically competent budding professionals for promising careers in the\n",
      "trading industry.\n",
      "\n",
      "\n",
      "\n",
      "I request you to forward the enclosed brochure to the students’ community.\n",
      "\n",
      "Thanks and Regards\n",
      "\n",
      "Dr. (Mrs) P KRISHNA PRASANNA\n",
      "Associate Professor,\n",
      "Department of Management Studies,\n",
      "IIT Madras\n",
      "pkp@iitm.ac.in\n",
      "+914422574571 (Off, direct)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "similar_doc1 = retrieval(doc1)\n",
    "print(similar_doc1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = '''This is to bring to your attention that due to an unfortunate technical difficulty from the SDM Hospital side, the blood donation camp is being postponed to next Saturday,  that is, October 19th, 2019. All those who are interested and haven't registered yet may fill the form by next Thursday.\n",
    "\n",
    "Thanks and Regards\n",
    "Soma Siddhartha \n",
    "General Secretary Hostel Affairs \n",
    "On Wed, Oct 9, 2019, 5:35 PM General Secretary Hostel Affairs <gsha@iitdh.ac.in> wrote:\n",
    "Dear all\n",
    "\n",
    "It is very disheartening that we have received only 29 registrations for the blood donation camp. To successfully arrange the blood donation camp we require at least 80 participants to take part in the camp. Blood is the most precious gift that anyone can give to another person — the gift of life. I request everyone to come forward to this noble cause and take up this opportunity to contribute to the wellbeing of society.  Please fill-up the form attached with the previous mail by tomorrow 12 pm to participate in the Blood Donation Camp  Also, please find attached the posters for blood donation camp, why blood donation and some of the important dos and don'ts for donating blood respectively.\n",
    "\n",
    "     Why one should donate blood!\n",
    "\n",
    " Dos and Don'ts for donating blood!\n",
    "\n",
    "Thanks and regards\n",
    "\n",
    "Soma Siddhartha\n",
    "General Secretary Hostel Affairs\n",
    "On Sun, Oct 6, 2019 at 10:49 AM General Secretary Hostel Affairs <gsha@iitdh.ac.in> wrote:\n",
    "Hello Everyone!\n",
    "I'm glad to announce that IIT Dharwad, in coordination with Sri Dharmasthala Manjunatheshwara  (SDM) Hospital and the Civil Hospital, is going to organize a Blood Donation Camp on October 12th, 2019 from 9:30 am to 2 pm. Doctors and trained professionals from SDM Hospital and the Civil Hospital will be coming to our campus and begin the Blood Donation process from 9:30 am. All those who are interested in participating in the Blood Donation Camp may fill the form below. Please find the rules and guidelines for blood donation in the attached forms, and once registered, please make sure to not to miss the camp. \n",
    "Posters, instructions and other dos and don'ts for blood donation will be shared over the next few days. Until then, stay healthy, and don't miss this opportunity! \n",
    "\n",
    "One Donation Can Save More than One Life! \n",
    "\n",
    "Soma Siddhartha\n",
    "General Secretary Hostel Affairs\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: HOLIKA DAHAN EVENT\n",
      "Text: On the auspicious occasion of Holika Dahan .All  are cordially invited to\n",
      "event.\n",
      "Please report near bhoopali mess around 10:30 pm\n",
      "Waiting for your presence.\n",
      "\n",
      "Regards\n",
      "Abhay Sahu\n",
      "Cricket Secy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "similar_doc2 = retrieval(doc2)\n",
    "print(similar_doc2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Re: Night Cricket Tournament: Vote Of Thanks\n",
      "Text: Dear Sahu,\n",
      "\n",
      "I am happy to note that the event went very well and you all made it a\n",
      "great success.  The mail is also very well drafted.  Keep it up on that\n",
      "note. It is really great to have this positive note before the end semester\n",
      "examination.\n",
      "\n",
      "Yours sincerely,\n",
      "Prabhu\n",
      "\n",
      "Professor in Mechanical Engineering, Larsen and Toubro Chair Professor,\n",
      "Dean (Academic Programme and Student Welfare)\n",
      "IIT Dharwad\n",
      "On deputation from IIT Bombay\n",
      "\n",
      "\n",
      "On Tue, Apr 9, 2019 at 5:48 AM ABHAY SAHU <160030020@iitdh.ac.in> wrote:\n",
      "\n",
      "> Dear all,\n",
      "> I feel great honour in extending my thanks to all involved in this\n",
      "> successfully concluded tournament.First of all, I thank Director, Dean\n",
      "> Student Welfare and Sports officer for their encouragement and approving of\n",
      "> the event.\n",
      "> I sincerely thank Vishal Sir, for giving me the various ideas and for\n",
      "> being always beside me to tackle any situation.His wise words have prompted\n",
      "> to visualize sports from a new perspective.They have been imprinted deep\n",
      "> and will continue to resound as we tread forward in our journey of life.\n",
      "> This event would not have been a grand success without such overwhelming\n",
      "> participation of all nine teams.I thank the captains accompanying their\n",
      "> respective teams.I thank all the participants for their display of sportive\n",
      "> talent and sportsmanship.\n",
      "> I am all admiration for Umpires, Commentators, Scorers, Support Staff and\n",
      "> Volunteers for their dedication and hard work.\n",
      "> **Volunteers- 1st year - Mohit, Aditya, Yogesh, Parth, Rahul, Nishant,\n",
      "> Nikhil, Himanshu, Sarat\n",
      "> 2nd year- Vipul, Indrajeet, Ashish, Kunal, Amar, Nandivardhan\n",
      "> 3rd year- Aman, Abhishek, Anand, Avinash, Sanjay, Gaurav, Jagdish, Ritik,\n",
      "> Lokesh, Chitransh, Puneet, Samveed, Rishi, Arpit **\n",
      "> As you all know it rained on 5th April, the ground was so wet that a match\n",
      "> would not have been possible, but these supportive people showed such\n",
      "> courage and character that within one and a half hour, the ground condition\n",
      "> became playable.That really showed their dedication and hard work towards\n",
      "> the success of this tournament.\n",
      "> I want to especially thank our volunteers from cultural and photography\n",
      "> department- Mehul, Swakath, Hawa, Abhishek for their cultural extravaganza\n",
      "> presented throughout the event. A blend of Culture and Sports, how good!\n",
      "> You audience have been so nice and loud. A big thank you to all of you. It\n",
      "> really set the standard for all future tournaments and will obviously be a\n",
      "> challenging trend to follow.\n",
      "> We look forward to your enthusiastic support in upcoming events.We really\n",
      "> had a great time together.\n",
      "> A weekend passed like a moment.\n",
      ">\n",
      "> Thank you\n",
      "> Abhay Sahu\n",
      ">\n",
      ">\n",
      ">\n",
      ">\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(similar_doc2[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
