{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import re\n",
    "import gensim\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pyLDAvis.gensim\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "from gensim.test.utils import datapath\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing(doc):\n",
    "    \"\"\"\n",
    "    Convert a document into a list of lowercase tokens, ignoring tokens that are too short or too long.\n",
    "    \"\"\"\n",
    "    return gensim.utils.simple_preprocess(str(doc), deacc=True, min_len=4, max_len=15)\n",
    "\n",
    "\n",
    "def preprocessed_doc(doc):\n",
    "    \"\"\"\n",
    "    Preprocessing a single document\n",
    "    \"\"\"   \n",
    "    doc = re.sub(r'https?\\S*\\s?', '', doc)\n",
    "    doc = re.sub(r'\\s\\S*\\.edu\\S*\\s?', ' ', doc)\n",
    "    doc = re.sub(r'\\s\\S*\\.com\\S*\\s?', ' ', doc)\n",
    "    doc = re.sub(r'www\\.\\S*\\s?', '', doc)\n",
    "    doc = re.sub(r'\\S*@\\S*\\s?', '', doc)\n",
    "    doc = re.sub(r\"\\'\", \"\", doc)\n",
    "    doc = re.sub(r'\\s+', ' ', doc)  \n",
    "    doc = tokenizing(doc)\n",
    "    return doc\n",
    "    \n",
    "    \n",
    "def preprocessed_data(data):\n",
    "    \"\"\"\n",
    "    Preprocessing the entire data (list of documents)\n",
    "    \"\"\"\n",
    "    return [preprocessed_doc(doc) for doc in data]\n",
    "\n",
    "\n",
    "df = pandas.read_json('data/newsgroups.json')\n",
    "data = df.content.values.tolist()\n",
    "tokenized_data = preprocessed_data(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-5783f8e18e55>, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-5783f8e18e55>\"\u001b[0;36m, line \u001b[0;32m23\u001b[0m\n\u001b[0;31m    'email', 'internet', 'problems', 'youre', '-PRON-', '_']]\u001b[0m\n\u001b[0m                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "stopwords_ = ['that', 'from', 'this', 'have', 'with', 'subject', 'they', 'lines', 'organization',\n",
    "              'what', 'will', 'there', 'would', 'about', 'writes', 'your', 'article', 'some',\n",
    "              'which', 'were', 'more', 'people', 'like', 'dont', 'when', 'just', 'university',\n",
    "              'posting', 'their', 'other', 'know', 'only', 'host', 'them', 'nntp', 'than', 'been',\n",
    "              'think', 'also', 'does', 'time', 'then', 'good', 'these', 'well', 'should', 'could',\n",
    "              'because', 'even', 'very', 'into', 'first', 'many', 'those', 'make', 'much',\n",
    "              'most', 'system', 'such', 'distribution', 'right', 'where', 'world', 'want', 'here',\n",
    "              'reply', 'used', 'being', 'said', 'over', 'anyone', 'after', 'same', 'need', 'work',\n",
    "              'something', 'problem', 'please', 'really', 'computer', 'since', 'back', 'believe',\n",
    "              'still', 'going', 'years', 'file', 'information', 'year', 'windows', 'help', 'mail',\n",
    "              'using', 'state', 'find', 'take', 'question', 'last', 'point', 'thanks', 'space',\n",
    "              'before', 'must', 'never', 'things', 'while', 'better', 'government', 'cant', 'might',\n",
    "              'both', 'number', 'read', 'sure', 'another', 'case', 'without', 'program', 'down',\n",
    "              'through', 'made', 'data', 'drive', 'software', 'long', 'available', 'part', 'under',\n",
    "              'david', 'thing', 'doesnt', 'someone', 'look', 'power', 'thats', 'between', 'little',\n",
    "              'version', 'come', 'didnt', 'however', 'each', 'public', 'around', 'anything', 'fact',\n",
    "              'science', 'best', 'give', 'true', 'every', 'probably', 'again', 'name', 'john',\n",
    "              'course', 'least', 'line', 'against', 'tell', 'seems', 'group', 'different',\n",
    "              'systems', 'great', 'enough', 'high', 'research', 'news', 'list', 'hard', 'real',\n",
    "              'says', 'second', 'jesus', 'possible', 'either', 'life', 'actually', 'game',\n",
    "              'though', 'support', 'card', 'technology', 'post', 'center', 'called', 'free',\n",
    "              'rather', 'nothing', 'access', 'next', 'team', 'chip', 'window', 'mean',\n",
    "              'email', 'internet', 'problems', 'youre', '-PRON-', '_']]\n",
    "\n",
    "with open('scripts/stopwords.txt', 'r') as fp:\n",
    "    for word in fp:\n",
    "        stopwords_.append(word.strip())\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(stopwords_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(doc):\n",
    "    \"\"\"\n",
    "    Removes stopwords from a document\n",
    "    \"\"\"\n",
    "    return [word for word in doc\n",
    "            if word not in stop_words]\n",
    "\n",
    "\n",
    "def make_bigrams(doc):\n",
    "    \"\"\"\n",
    "    Make bigrams of a document\n",
    "    \"\"\"\n",
    "    return bigram_mod[doc]\n",
    "\n",
    "\n",
    "def lemmatization(doc):\n",
    "    \"\"\"\n",
    "    Lemmatizes a document\n",
    "    \"\"\"\n",
    "    doc = nlp(\" \".join(doc))\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "\n",
    "def processed_doc(doc):\n",
    "    \"\"\"\n",
    "    Processing a document\n",
    "    \"\"\"\n",
    "    doc = remove_stopwords(doc)\n",
    "    doc = make_bigrams(doc)\n",
    "    doc = lemmatization(doc)\n",
    "    doc = remove_stopwords(doc)\n",
    "    return doc\n",
    "\n",
    "\n",
    "def processed_data(data):\n",
    "    \"\"\"\n",
    "    Return lemmatized data\n",
    "    \"\"\"\n",
    "    return [processed_doc(doc) for doc in data]\n",
    "\n",
    "\n",
    "bigram = gensim.models.Phrases(tokenized_data, min_count=5, threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "lemmatized_data = processed_data(tokenized_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_topics = 20\n",
    "lda_model = gensim.models.LdaModel.load(datapath('20newsgroups_bow'))\n",
    "id2word = lda_model.id2word\n",
    "corpus = [id2word.doc2bow(text) for text in lemmatized_data]\n",
    "\n",
    "# tfidf = gensim.models.TfidfModel(corpus)\n",
    "# corpus = tfidf[corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics():\n",
    "    \"\"\"\n",
    "    Prints top 30 words from each topic\n",
    "    \"\"\"\n",
    "    pprint(lda_model.show_topics(num_of_topics, 30))\n",
    "\n",
    "\n",
    "def get_doc_topic_distribution(index):\n",
    "    \"\"\"\n",
    "    Get the topic distribution for a given document\n",
    "    \"\"\"\n",
    "    doc = corpus[index]\n",
    "    return lda_model.get_document_topics(doc)\n",
    "\n",
    "\n",
    "def get_topic_term():\n",
    "    \"\"\"\n",
    "    Get topic-term matrix\n",
    "    \"\"\"\n",
    "    topic_term_matrix = lda_model.get_topics()\n",
    "    print(len(topic_term_matrix), 'x', len(topic_term_matrix[0]))\n",
    "    return topic_term_matrix\n",
    "\n",
    "\n",
    "def get_term_topics(word_id):\n",
    "    \"\"\"\n",
    "    Get the most relevant topics to the given word\n",
    "    \"\"\"\n",
    "    print('Word is: ', id2word[word_id])\n",
    "    relevant_topics = lda_model.get_term_topics(word_id)\n",
    "    relevant_topic_ids = [topic_id[0] for topic_id in relevant_topics]\n",
    "    for topic_id in relevant_topic_ids:\n",
    "        print(show_topic(topic_id))\n",
    "    return relevant_topics\n",
    "\n",
    "\n",
    "def unseen_doc_topic_distribution(new_doc: list):\n",
    "    \"\"\"\n",
    "    Get topic distribution of a unseen document\n",
    "    \"\"\"\n",
    "    new_doc = [id2word.doc2bow(new_doc)]\n",
    "    topics = lda_model[new_doc]\n",
    "    return sorted(topics[0][0], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "def doc_topic_matrix():\n",
    "    \"\"\"\n",
    "    Get document topic matrix\n",
    "    \"\"\"\n",
    "    matrix = np.zeros(shape=(len(corpus), num_of_topics))\n",
    "    for i in range(len(corpus)):\n",
    "        topic_dist = get_doc_topic_distribution(i)\n",
    "        for topic, prob in topic_dist:\n",
    "            matrix[i][topic] = prob\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def jensen_shannon(query, matrix):\n",
    "    \"\"\"\n",
    "    This function implements a Jensen-Shannon similarity\n",
    "    between the input query (an LDA topic distribution for a document)\n",
    "    and the entire corpus of topic distributions.\n",
    "    It returns an array of length M where M is the number of documents in the corpus\n",
    "    \"\"\"\n",
    "    p = query[None, :].T\n",
    "    q = matrix.T\n",
    "    m = 0.5 * (p + q)\n",
    "    # entropy calculated KL divergence\n",
    "    return np.sqrt(0.5 * (entropy(p, m) + entropy(q, m)))\n",
    "\n",
    "\n",
    "def get_document(index):\n",
    "    \"\"\"\n",
    "    Index is list or num\n",
    "    \"\"\"\n",
    "    if isinstance(index, list):\n",
    "        docs = []\n",
    "        for i in index:\n",
    "            docs.append(data[i])\n",
    "        return docs\n",
    "    return data[index]\n",
    "\n",
    "\n",
    "def get_similar_docs(new_doc: list):\n",
    "    \"\"\"\n",
    "    This function implements the Jensen-Shannon distance above\n",
    "    and retruns the top k indices of the smallest jensen shannon distances\n",
    "    \"\"\"\n",
    "    topics = unseen_doc_topic_distribution(new_doc)\n",
    "    topic_dist = np.zeros(shape=(num_of_topics))\n",
    "    for topic, prob in topics:\n",
    "        topic_dist[topic] = prob\n",
    "    sims = jensen_shannon(topic_dist, doc_topic_matrix())\n",
    "    docs = list(sims.argsort()[:10])\n",
    "    return get_document(docs)\n",
    "\n",
    "\n",
    "def retrieval(doc):\n",
    "    doc = preprocessed_doc(doc)\n",
    "    doc = processed_doc(doc)\n",
    "    return get_similar_docs(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = '''\n",
    "Don and Ron it was the an \"off-night\" for the Leafs and the Devils \n",
    "were outplaying Toronto. Well, I BEG to differ....\n",
    "\n",
    "IMHO, Clark deserved to be a first star as much as Gilmour did. His\n",
    "fast breaks towards the net and the good opportunites that he\n",
    "created reminded me of the Clark of old. (But not to take any of the\n",
    "credit away from Gilmour).\n",
    "\n",
    "I think the Leafs are playing GREAT hockey. WHY? \n",
    "Well first look at their injury list which includes, Cullen, Ellet,\n",
    "Zezel, Macoun. Of course my question is this....how will the Leafs\n",
    "fare when they are once again \"healthy\" if they are playing this well\n",
    "so far??\n",
    "\n",
    "Second, just look at their standings, still second in defence,\n",
    "moved from 11th overall to 6th over in the last month, haven't lost\n",
    "at home in last 12 games, 8 game undefeated streak..etc.\n",
    "(BTW, am I wrong or was this Potvin's first shut-out? I can't \n",
    "remember him having any as of yet.)\n",
    "\n",
    "Well, as of April 3 we see that the race for first in the Norris\n",
    "has truly begun and it will be a VERY CLOSE race between Chicago and\n",
    "Toronto. And the best game of the season will probably be their last\n",
    "against each other. (is anyone lucky enough to have tickets to\n",
    "see this one?)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: gballent@hudson.UVic.CA (Greg  Ballentine)\n",
      "Subject: Re: plus minus stat\n",
      "Nntp-Posting-Host: hudson.uvic.ca\n",
      "Reply-To: gballent@hudson.UVic.CA\n",
      "Organization: University of Victoria, Victoria, BC, Canada\n",
      "Lines: 38\n",
      "\n",
      "\n",
      "In article 20009@ramsey.cs.laurentian.ca, maynard@ramsey.cs.laurentian.ca (Roger Maynard) writes:\n",
      ">In <1993Apr15.160450.27799@sol.UVic.CA> gballent@hudson.UVic.CA (Greg  Ballentine) writes:\n",
      "\n",
      ">>Gainey is the best defensive forward ever.  I stand by that assessment.\n",
      ">>He was a very good player who belongs in the hall of fame.  Did you\n",
      ">>ever watch him play? He never made a technical error.\n",
      ">\n",
      ">I watched him over his entire career.  I have NEVER seen a player, and that\n",
      ">includes Russell Courtnall and Davie Keon, screw up as many breakaways as\n",
      ">Bob Gainey.  And I will never forget the time Denis Potvin caught Gainey\n",
      ">with his head down.  You have been sold a bill of goods on Bob Gainey.\n",
      ">\n",
      ">Gainey was a plugger.  And when the press runs out of things to say about \n",
      ">the stars on dynasties they start to hype the pluggers.  Grant Fuhr, Essa\n",
      ">Tikkannen, Butch Goring, Bob Nystrom, Bob Gainey, Doug Jarvis, Derek\n",
      ">Sanderson, Wayne Cashman, Bob Baun, Bob Pulford, Ralph Backstrom, Henri\n",
      ">Richard, Dick Duff...and so on...\n",
      "\n",
      "These players all are pretty good players.  They are the depth that the\n",
      "dynasties had to win Stanley Cups.  They tend to be the very good second\n",
      "line guys- who would be first liners on most weaker clubs in the NHL.\n",
      "They were all important to their clubs.  Probably, several of these\n",
      "Stanley Cup winning teams would not have won the cups they did if it\n",
      "were not for the depth provided by these players.\n",
      "\n",
      "They compare to Rick Tocchet and Ron Francis of the Penguins.  Very good\n",
      "players who can lead lesser teams (Francis-Hartford, Tocchet-Philly) who\n",
      "provide the depth to the team that is currently best in the NHL.\n",
      "\n",
      "As a defensive forward, there have been none better than Bob Gainey.  That\n",
      "doesn't mean he was the best player (or even the best forward) the Canadians\n",
      "had at that time, but he was excellent at what he did.  Gainey could\n",
      "dominate games with his defence.  He didn't need to get goals to dominate.\n",
      "He shut down the opposition and was thus valuable.  There has never been\n",
      "anyone any better at doing this.  Not ever.\n",
      "\n",
      "Gregmeister\n",
      "\n"
     ]
    }
   ],
   "source": [
    "similar_doc1 = retrieval(doc1)\n",
    "print(similar_doc1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: ayim@leibniz.uwaterloo.ca (Alfred Yim)\n",
      "Subject: And... THEY'RE OFF!!!!!\n",
      "Keywords: Leafs Chicago\n",
      "Organization: University of Waterloo\n",
      "Lines: 39\n",
      "\n",
      "Well, I gotta tell ya,\n",
      "\n",
      "last night's Leafs game vs the Devils was a nail-bitter LET ME TELL YOU!\n",
      "It was a well played game by BOTH teams (I thought) but according to the\n",
      "Don and Ron it was the an \"off-night\" for the Leafs and the Devils \n",
      "were outplaying Toronto. Well, I BEG to differ....\n",
      "\n",
      "IMHO, Clark deserved to be a first star as much as Gilmour did. His\n",
      "fast breaks towards the net and the good opportunites that he\n",
      "created reminded me of the Clark of old. (But not to take any of the\n",
      "credit away from Gilmour).\n",
      "\n",
      "I think the Leafs are playing GREAT hockey. WHY? \n",
      "Well first look at their injury list which includes, Cullen, Ellet,\n",
      "Zezel, Macoun. Of course my question is this....how will the Leafs\n",
      "fare when they are once again \"healthy\" if they are playing this well\n",
      "so far??\n",
      "\n",
      "Second, just look at their standings, still second in defence,\n",
      "moved from 11th overall to 6th over in the last month, haven't lost\n",
      "at home in last 12 games, 8 game undefeated streak..etc.\n",
      "(BTW, am I wrong or was this Potvin's first shut-out? I can't \n",
      "remember him having any as of yet.)\n",
      "\n",
      "Well, as of April 3 we see that the race for first in the Norris\n",
      "has truly begun and it will be a VERY CLOSE race between Chicago and\n",
      "Toronto. And the best game of the season will probably be their last\n",
      "against each other. (is anyone lucky enough to have tickets to\n",
      "see this one?)\n",
      "\n",
      "Coming to the stretch and still a ROAR'IN!!!!!\n",
      "Go LEAFS Go!!!!\n",
      "-- \n",
      "****************************************** \n",
      "*  Alfred (Yong-Jeh) Yim                 *   Toronto wins the  \n",
      "*  4B Mathematics (Actuarial Science)    *     (   ?    )    CUP.\n",
      "*  University of Waterloo, Canada.       *  i like \"coca-cola\" idea personally\n",
      "*  E-mail: ayim@descartes.waterloo.edu   *  \n",
      "*****************************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(similar_doc1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
