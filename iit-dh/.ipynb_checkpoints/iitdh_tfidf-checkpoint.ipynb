{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import re\n",
    "import gensim\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pyLDAvis.gensim\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "from gensim.test.utils import datapath\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_lines = [\"It's not an attachment -- it's stored online. To open this item, just clickthe link above.\",\n",
    "                \"I've shared an item with you:\"]\n",
    "\n",
    "\n",
    "def tokenizing(doc):\n",
    "    \"\"\"\n",
    "    Convert a document into a list of lowercase tokens, ignoring tokens that are too short or too long.\n",
    "    \"\"\"\n",
    "    return gensim.utils.simple_preprocess(str(doc), deacc=True, min_len=4, max_len=15)\n",
    "\n",
    "\n",
    "def preprocessed_doc(doc):\n",
    "    \"\"\"\n",
    "    Preprocessing a single document\n",
    "    \"\"\"\n",
    "    for line in remove_lines:\n",
    "        doc = doc.replace(line, '')\n",
    "    \n",
    "    doc = re.sub(r'https?\\S*\\s?', '', doc)\n",
    "    doc = re.sub(r'\\s\\S*\\.ac\\.in\\S*\\s?', ' ', doc)\n",
    "    doc = re.sub(r'\\s\\S*\\.edu\\S*\\s?', ' ', doc)\n",
    "    doc = re.sub(r'\\s\\S*\\.com\\S*\\s?', ' ', doc)\n",
    "    doc = re.sub(r'www\\.\\S*\\s?', '', doc)\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc)\n",
    "    doc = re.sub(r'\\s+', ' ', doc)    \n",
    "    doc = tokenizing(doc)\n",
    "    return doc\n",
    "    \n",
    "    \n",
    "def preprocessed_data(data):\n",
    "    \"\"\"\n",
    "    Preprocessing the entire data (list of documents)\n",
    "    \"\"\"\n",
    "    return [preprocessed_doc(doc) for doc in data]\n",
    "\n",
    "\n",
    "df = pandas.read_json('data/processed_iitdh_broadcast-3.json')\n",
    "data = df.content.values.tolist()\n",
    "tokenized_data = preprocessed_data(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_ = ['dharwad', 'regard', 'iit', 'please', 'student',\n",
    "              'institute', 'write', 'thank', 'form', 'technology',\n",
    "              'would', 'year', 'time', 'follow', 'email', 'room', 'fill', 'date',\n",
    "              'also', 'indian', 'engineering', 'give', 'get', 'day', 'work', 'india',\n",
    "              'detail', 'mail', 'interested', 'use', 'request', 'guy', 'may',\n",
    "              'not', 'link', 'like', 'take', 'make', 'still', 'since', 'keep',\n",
    "              'secretary', 'come', 'one', 'professor', 'today', 'good',\n",
    "              'find', 'tomorrow', 'first', 'send', 'system', 'start',\n",
    "              'information', 'member', 'prof', 'part', 'regretted' 'registrar'\n",
    "              'new', 'venue', 'attend', 'kindly', 'aug', '-PRON-',\n",
    "              'image', 'provide', 'well', 'visit', 'do', 'inconvenience'\n",
    "              'assistant_professor', 'want', 'contact', 'go', 'meet',\n",
    "              'invite', 'name', 'need', 'opportunity', 'attach',\n",
    "              'everyone', 'google', 'receive', 'conduct', 'great', 'note', 'affair',\n",
    "              'available', 'number', 'august', 'many', 'college', 'share',\n",
    "              'help', 'programme', 'walmi_campus', 'belur_industrial',\n",
    "              'know', 'hold', 'participation', 'march', 'group', 'walmi'\n",
    "              'rule', 'see', 'learn', 'hello_everyone', 'indore', 'campus'\n",
    "              'challenge', 'phd', 'present', 'people', 'saturday', 'open', 'hope',\n",
    "              'april', 'jan', 'online', 'issue', 'require', 'play', 'thing',\n",
    "              'back', 'faculty', 'dean', 'summer', 'join', 'next', 'inform',\n",
    "              'base', 'topic', 'hello', 'session', 'idea', 'model', 'participant',\n",
    "              'post', 'last', 'change', 'round', 'karnataka_india', 'title', 'list',\n",
    "              'two', 'bring', 'access', 'app', 'include', 'click', 'sincerely', 'learn'\n",
    "              'view', 'hour', 'close', 'question', 'create', 'third', 'front', 'arpit', 'agrawal', 'btech',\n",
    "              'high', 'week', 'gentle_reminder', 'september', 'service', 'area', 'morning',\n",
    "              'timing', 'inter', 'from', 'subject', 'text', 'fwd', 'forwarded', 'message', 'dear', 'f',\n",
    "              'iitdh', 'iitg', 'deputation', 'three', 'become', 'karnataka', 'select', 'wbw_prasanna',\n",
    "              'unsubscribe', 'pron', 'hey', 'cse', 'january', 'february', 'march', 'april', 'june', 'july',\n",
    "              'august', 'september', 'october', 'november', 'december', 'larsen', 'tourbo', 'tourbo_chair',\n",
    "              'bombay', 'lot', 'sit', 'try', 'ive_invite', 'till', 'every', 'never', 'near', 'welfare', 'techf',\n",
    "              'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'secy', 'could',\n",
    "              'student', 'department', 'first', 'second', 'four', 'feel', 'reminder', 'gentle',  'sure',\n",
    "              'sign', 'photo', 'welcome', 'assistant', 'already', 'sorry', 'small', 'attachment', 'ever']\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(stopwords_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(tokenized_data, min_count=5, threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "\n",
    "def remove_stopwords(doc):\n",
    "    \"\"\"\n",
    "    Removes stopwords from a document\n",
    "    \"\"\"\n",
    "    return [word for word in doc\n",
    "            if word not in stop_words]\n",
    "\n",
    "\n",
    "def make_bigrams(doc):\n",
    "    \"\"\"\n",
    "    Make bigrams of a document\n",
    "    \"\"\"\n",
    "    return bigram_mod[doc]\n",
    "\n",
    "\n",
    "def lemmatization(doc):\n",
    "    \"\"\"\n",
    "    Lemmatizes a document\n",
    "    \"\"\"\n",
    "    doc = nlp(\" \".join(doc))\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "\n",
    "def processed_doc(doc):\n",
    "    \"\"\"\n",
    "    Processing a document\n",
    "    \"\"\"\n",
    "    doc = remove_stopwords(doc)\n",
    "    doc = make_bigrams(doc)\n",
    "    doc = lemmatization(doc)\n",
    "    doc = remove_stopwords(doc)\n",
    "    return doc\n",
    "\n",
    "\n",
    "def processed_data(data):\n",
    "    \"\"\"\n",
    "    Return lemmatized data\n",
    "    \"\"\"\n",
    "    return [processed_doc(doc) for doc in data]\n",
    "\n",
    "lemmatized_data = processed_data(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_topics = 10\n",
    "lda_model = gensim.models.LdaModel.load(datapath('iitdh_bow_10_0.01_0.61'))\n",
    "id2word = gensim.corpora.Dictionary(lemmatized_data)\n",
    "corpus = corpus = [id2word.doc2bow(text) for text in lemmatized_data]\n",
    "\n",
    "tfidf = gensim.models.TfidfModel(corpus)\n",
    "corpus = tfidf[corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics():\n",
    "    \"\"\"\n",
    "    Prints top 30 words from each topic\n",
    "    \"\"\"\n",
    "    pprint(lda_model.show_topics(num_of_topics, 30))\n",
    "\n",
    "\n",
    "def get_doc_topic_distribution(index):\n",
    "    \"\"\"\n",
    "    Get the topic distribution for a given document\n",
    "    \"\"\"\n",
    "    doc = corpus[index]\n",
    "    return lda_model.get_document_topics(doc)\n",
    "\n",
    "\n",
    "def get_topic_term():\n",
    "    \"\"\"\n",
    "    Get topic-term matrix\n",
    "    \"\"\"\n",
    "    topic_term_matrix = lda_model.get_topics()\n",
    "    print(len(topic_term_matrix), 'x', len(topic_term_matrix[0]))\n",
    "    return topic_term_matrix\n",
    "\n",
    "\n",
    "def get_term_topics(word_id):\n",
    "    \"\"\"\n",
    "    Get the most relevant topics to the given word\n",
    "    \"\"\"\n",
    "    print('Word is: ', id2word[word_id])\n",
    "    relevant_topics = lda_model.get_term_topics(word_id)\n",
    "    relevant_topic_ids = [topic_id[0] for topic_id in relevant_topics]\n",
    "    for topic_id in relevant_topic_ids:\n",
    "        print(show_topic(topic_id))\n",
    "    return relevant_topics\n",
    "\n",
    "\n",
    "def unseen_doc_topic_distribution(new_doc: list):\n",
    "    \"\"\"\n",
    "        Get topic distribution of a unseen document\n",
    "    \"\"\"\n",
    "    new_doc = [id2word.doc2bow(new_doc)]\n",
    "    topics = lda_model[new_doc]\n",
    "    return sorted(topics[0][0], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "def doc_topic_matrix():\n",
    "    \"\"\"\n",
    "    Get document topic matrix\n",
    "    \"\"\"\n",
    "    matrix = np.zeros(shape=(len(corpus), num_of_topics))\n",
    "    for i in range(len(corpus)):\n",
    "        topic_dist = get_doc_topic_distribution(i)\n",
    "        for topic, prob in topic_dist:\n",
    "            matrix[i][topic] = prob\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def jensen_shannon(query, matrix):\n",
    "    \"\"\"\n",
    "    This function implements a Jensen-Shannon similarity\n",
    "    between the input query (an LDA topic distribution for a document)\n",
    "    and the entire corpus of topic distributions.\n",
    "    It returns an array of length M where M is the number of documents in the corpus\n",
    "    \"\"\"\n",
    "    p = query[None, :].T\n",
    "    q = matrix.T\n",
    "    m = 0.5 * (p + q)\n",
    "    # entropy calculated KL divergence\n",
    "    return np.sqrt(0.5 * (entropy(p, m) + entropy(q, m)))\n",
    "\n",
    "\n",
    "def get_document(index):\n",
    "    \"\"\"\n",
    "    Index is list or num\n",
    "    \"\"\"\n",
    "    if isinstance(index, list):\n",
    "        docs = []\n",
    "        for i in index:\n",
    "            docs.append(data[i])\n",
    "        return docs\n",
    "    return data[index]\n",
    "\n",
    "\n",
    "def get_similar_docs(new_doc: list):\n",
    "    \"\"\"\n",
    "    This function implements the Jensen-Shannon distance above\n",
    "    and retruns the top k indices of the smallest jensen shannon distances\n",
    "    \"\"\"\n",
    "    topics = unseen_doc_topic_distribution(new_doc)\n",
    "    topic_dist = np.zeros(shape=(num_of_topics))\n",
    "    for topic, prob in topics:\n",
    "        topic_dist[topic] = prob\n",
    "    sims = jensen_shannon(topic_dist, doc_topic_matrix())\n",
    "    docs = list(sims.argsort()[:10])\n",
    "    return get_document(docs)\n",
    "\n",
    "\n",
    "def retrieval(doc):\n",
    "    doc = preprocessed_doc(doc)\n",
    "    doc = processed_doc(doc)\n",
    "    return get_similar_docs(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = '''Dear Sir/Madam,\n",
    "Invitation for Diwali 2019 celebration\n",
    "\n",
    "Namaskar,\n",
    "\n",
    "On this auspicious occasion of the festival of lights, let us all \n",
    "come together to celebrate this rich blend of cultures. We plan to make this \n",
    "festive weekend of Diwali joyful, fun and memorable. We have a whole lot of events \n",
    "lined up for all of you and hopefully all of us can have a great time. \n",
    "The agenda for the weekend i.e., this Saturday( 26-10-19) and Sunday(27-10-19) is as follows-\n",
    "\n",
    "Regards,\n",
    "Diwali Organising team,\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: DevHack WhatsApp Group\n",
      "Text: [image: unnamed.png]\n",
      "üëèüëèStop those mosquitoes that are buzzing,\n",
      "and catch some sleep tonight\n",
      "as* DevHack *is coming!\n",
      "\n",
      "Click on this link <https://chat.whatsapp.com/HGy0O8FJb9m8BZoxQSiLeb> to\n",
      "join the *DevHack WhatsApp group *for quick communication during the\n",
      "hackathon.\n",
      "\n",
      "\n",
      "üò™üò™Buenas Noche\n",
      "\n",
      "Sonu Sourav\n",
      "DevHack Team\n",
      "\n"
     ]
    }
   ],
   "source": [
    "similar_docs = retrieval(doc)\n",
    "print(similar_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Re: Registration for the Co-Op program\n",
      "Text: Some answers to general queries regarding the co-op program:\n",
      "The co-op program is for the fourth year students since they are having a\n",
      "BTP this semester.\n",
      "Note that it is for the coming semester (August to November 2019). For next\n",
      "semester, the form has to be filled again.\n",
      "For students who has been allocated a btp and who are  opting for the Co-Op\n",
      "program, the corresponding project will be allocated to another student at\n",
      "the discretion of the faculty members.\n",
      "Try to fill the form as soon as possible. Send the scanned pdf of the\n",
      "filled form through mail to CDC Support.\n",
      "\n",
      "Thanking you\n",
      "Yours sincerely\n",
      "Riya Toteja\n",
      "CDC Support\n",
      "\n",
      "\n",
      "On Thu, Jul 25, 2019 at 9:53 AM Career Development Cell Support <\n",
      "cdc.support@iitdh.ac.in> wrote:\n",
      "\n",
      "> Hello Everyone\n",
      ">\n",
      "> This is to inform all the students who are currently on an internship. If\n",
      "> you wish to convert your   internship to a Co-Op project for the next\n",
      "> semester,  discuss with the company and also identify a faculty mentor from\n",
      "> IITDh under whose mentorship you wish to do the project. Once you manage to\n",
      "> do this, you can ask the company representative to fill and sign this form,\n",
      "> and also send a brief description of the project and the objectives. The\n",
      "> form after being filled has to be submitted to cdc both hardcopy and\n",
      "> scanned copy by email.\n",
      "> PFA the  form that a company has to fill for registration to the Co-Op\n",
      "> program.\n",
      ">\n",
      "> In case of any doubts please feel free to contact us.\n",
      ">\n",
      "> Thanking you\n",
      "> Yours sincerely\n",
      "> Riya\n",
      "> CDC Support\n",
      "> IIT Dharwad\n",
      ">\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(similar_docs[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
