{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import re\n",
    "import gensim\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pyLDAvis.gensim\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "from gensim.test.utils import datapath\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_lines = [\"It's not an attachment -- it's stored online. To open this item, just clickthe link above.\",\n",
    "                \"I've shared an item with you:\"]\n",
    "\n",
    "\n",
    "def tokenizing(doc):\n",
    "    \"\"\"\n",
    "        Convert a document into a list of lowercase tokens, ignoring tokens that are too short or too long.\n",
    "    \"\"\"\n",
    "    return gensim.utils.simple_preprocess(str(doc), deacc=True, min_len=4, max_len=15)\n",
    "\n",
    "\n",
    "def preprocessed_doc(doc):\n",
    "    \"\"\"\n",
    "        Preprocessing a single document\n",
    "    \"\"\"\n",
    "    for line in remove_lines:\n",
    "        doc = doc.replace(line, '')\n",
    "    \n",
    "    doc = re.sub(r'https?\\S*\\s?', '', doc)\n",
    "    doc = re.sub(r'\\s\\S*\\.ac\\.in\\S*\\s?', ' ', doc)\n",
    "    doc = re.sub(r'\\s\\S*\\.edu\\S*\\s?', ' ', doc)\n",
    "    doc = re.sub(r'\\s\\S*\\.com\\S*\\s?', ' ', doc)\n",
    "    doc = re.sub(r'www\\.\\S*\\s?', '', doc)\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc)\n",
    "    doc = re.sub(r'\\s+', ' ', doc)    \n",
    "    doc = tokenizing(doc)\n",
    "    return doc\n",
    "    \n",
    "    \n",
    "def preprocessed_data(data):\n",
    "    \"\"\"\n",
    "        Preprocessing the entire data (list of documents)\n",
    "    \"\"\"\n",
    "    return [preprocessed_doc(doc) for doc in data]\n",
    "\n",
    "\n",
    "df = pandas.read_json('data/processed_iitdh_broadcast-3.json')\n",
    "data = df.content.values.tolist()\n",
    "tokenized_data = preprocessed_data(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_ = ['dharwad', 'regard', 'iit', 'please', 'student',\n",
    "              'institute', 'write', 'thank', 'form', 'technology',\n",
    "              'would', 'year', 'time', 'follow', 'email', 'room', 'fill', 'date',\n",
    "              'also', 'indian', 'engineering', 'give', 'get', 'day', 'work', 'india',\n",
    "              'detail', 'mail', 'interested', 'use', 'request', 'guy', 'may',\n",
    "              'not', 'link', 'like', 'take', 'make', 'still', 'since', 'keep',\n",
    "              'secretary', 'come', 'one', 'professor', 'today', 'good',\n",
    "              'find', 'tomorrow', 'first', 'send', 'system', 'start',\n",
    "              'information', 'member', 'prof', 'part', 'regretted' 'registrar'\n",
    "              'new', 'venue', 'attend', 'kindly', 'aug', '-PRON-',\n",
    "              'image', 'provide', 'well', 'visit', 'do', 'inconvenience'\n",
    "              'assistant_professor', 'want', 'contact', 'go', 'meet',\n",
    "              'invite', 'name', 'need', 'opportunity', 'attach',\n",
    "              'everyone', 'google', 'receive', 'conduct', 'great', 'note', 'affair',\n",
    "              'available', 'number', 'august', 'many', 'college', 'share',\n",
    "              'help', 'programme', 'walmi_campus', 'belur_industrial',\n",
    "              'know', 'hold', 'participation', 'march', 'group', 'walmi'\n",
    "              'rule', 'see', 'learn', 'hello_everyone', 'indore', 'campus'\n",
    "              'challenge', 'phd', 'present', 'people', 'saturday', 'open', 'hope',\n",
    "              'april', 'jan', 'online', 'issue', 'require', 'play', 'thing',\n",
    "              'back', 'faculty', 'dean', 'summer', 'join', 'next', 'inform',\n",
    "              'base', 'topic', 'hello', 'session', 'idea', 'model', 'participant',\n",
    "              'post', 'last', 'change', 'round', 'karnataka_india', 'title', 'list',\n",
    "              'two', 'bring', 'access', 'app', 'include', 'click', 'sincerely', 'learn'\n",
    "              'view', 'hour', 'close', 'question', 'create', 'third', 'front', 'arpit', 'agrawal', 'btech',\n",
    "              'high', 'week', 'gentle_reminder', 'september', 'service', 'area', 'morning',\n",
    "              'timing', 'inter', 'from', 'subject', 'text', 'fwd', 'forwarded', 'message', 'dear', 'f',\n",
    "              'iitdh', 'iitg', 'deputation', 'three', 'become', 'karnataka', 'select', 'wbw_prasanna',\n",
    "              'unsubscribe', 'pron', 'hey', 'cse', 'january', 'february', 'march', 'april', 'june', 'july',\n",
    "              'august', 'september', 'october', 'november', 'december', 'larsen', 'tourbo', 'tourbo_chair',\n",
    "              'bombay', 'lot', 'sit', 'try', 'ive_invite', 'till', 'every', 'never', 'near', 'welfare', 'techf',\n",
    "              'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'secy', 'could',\n",
    "              'student', 'department', 'first', 'second', 'four', 'feel', 'reminder', 'gentle',  'sure',\n",
    "              'sign', 'photo', 'welcome', 'assistant', 'already', 'sorry', 'small', 'attachment', 'ever']\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(stopwords_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(doc):\n",
    "    \"\"\"\n",
    "        Removes stopwords from a document\n",
    "    \"\"\"\n",
    "    return [word for word in doc\n",
    "            if word not in stop_words]\n",
    "\n",
    "\n",
    "def make_bigrams(doc):\n",
    "    \"\"\"\n",
    "        Make bigrams of a document\n",
    "    \"\"\"\n",
    "    return bigram_mod[doc]\n",
    "\n",
    "\n",
    "def lemmatization(doc):\n",
    "    \"\"\"\n",
    "        Lemmatizes a document\n",
    "    \"\"\"\n",
    "    doc = nlp(\" \".join(doc))\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "\n",
    "def processed_doc(doc):\n",
    "    \"\"\"\n",
    "        Processing a document\n",
    "    \"\"\"\n",
    "    doc = remove_stopwords(doc)\n",
    "    doc = make_bigrams(doc)\n",
    "    doc = lemmatization(doc)\n",
    "    doc = remove_stopwords(doc)\n",
    "    return doc\n",
    "\n",
    "\n",
    "def processed_data(data):\n",
    "    \"\"\"\n",
    "        Return lemmatized data\n",
    "    \"\"\"\n",
    "    return [processed_doc(doc) for doc in data]\n",
    "\n",
    "\n",
    "bigram = gensim.models.Phrases(tokenized_data, min_count=5, threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "lemmatized_data = processed_data(tokenized_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_topics = 10\n",
    "lda_model = gensim.models.ldamodel.LdaModel.load(datapath())\n",
    "id2word = gensim.corpora.Dictionary(lemmatized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics():\n",
    "    \"\"\"\n",
    "        Prints top 30 words from each topic\n",
    "    \"\"\"\n",
    "    pprint(lda_model.show_topics(num_of_topics, 30))\n",
    "\n",
    "\n",
    "def get_doc_topic_distribution(index):\n",
    "    \"\"\"\n",
    "        Get the topic distribution for a given document\n",
    "    \"\"\"\n",
    "    doc = corpus[index]\n",
    "    return lda_model.get_document_topics(doc)\n",
    "\n",
    "\n",
    "def get_topic_term():\n",
    "    \"\"\"\n",
    "        Get topic-term matrix\n",
    "    \"\"\"\n",
    "    topic_term_matrix = lda_model.get_topics()\n",
    "    print(len(topic_term_matrix), 'x', len(topic_term_matrix[0]))\n",
    "    return topic_term_matrix\n",
    "\n",
    "\n",
    "def get_term_topics(word_id):\n",
    "    \"\"\"\n",
    "        Get the most relevant topics to the given word\n",
    "    \"\"\"\n",
    "    print('Word is: ', id2word[word_id])\n",
    "    relevant_topics = lda_model.get_term_topics(word_id)\n",
    "    relevant_topic_ids = [topic_id[0] for topic_id in relevant_topics]\n",
    "    for topic_id in relevant_topic_ids:\n",
    "        print(show_topic(topic_id))\n",
    "    return relevant_topics\n",
    "\n",
    "\n",
    "def unseen_doc_topic_distribution(new_doc: list):\n",
    "    \"\"\"\n",
    "        Get topic distribution of a unseen document\n",
    "    \"\"\"\n",
    "    new_doc = [id2word.doc2bow(new_doc)]\n",
    "    topics = lda_model[new_doc]\n",
    "    return sorted(topics[0], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "def doc_topic_matrix():\n",
    "    \"\"\"\n",
    "        Get document topic matrix\n",
    "    \"\"\"\n",
    "    matrix = np.zeros(shape=(len(corpus), num_of_topics))\n",
    "    for i in range(len(corpus)):\n",
    "        topic_dist = get_doc_topic_distribution(i)\n",
    "        for topic, prob in topic_dist:\n",
    "            matrix[i][topic] = prob\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def jensen_shannon(query, matrix):\n",
    "    \"\"\"\n",
    "        This function implements a Jensen-Shannon similarity\n",
    "        between the input query (an LDA topic distribution for a document)\n",
    "        and the entire corpus of topic distributions.\n",
    "        It returns an array of length M where M is the number of documents in the corpus\n",
    "    \"\"\"\n",
    "    p = query[None, :].T\n",
    "    q = matrix.T\n",
    "    m = 0.5 * (p + q)\n",
    "    # entropy calculated KL divergence\n",
    "    return np.sqrt(0.5 * (entropy(p, m) + entropy(q, m)))\n",
    "\n",
    "\n",
    "def get_document(index):\n",
    "    \"\"\"\n",
    "        index is list or num\n",
    "    \"\"\"\n",
    "    if isinstance(index, list):\n",
    "        docs = []\n",
    "        for i in index:\n",
    "            docs.append(data[i])\n",
    "        return docs\n",
    "    return data[index]\n",
    "\n",
    "\n",
    "def get_similar_docs(new_doc: list):\n",
    "    \"\"\"\n",
    "        This function implements the Jensen-Shannon distance above\n",
    "        and retruns the top k indices of the smallest jensen shannon distances\n",
    "    \"\"\"\n",
    "    topics = unseen_doc_topic_distribution(new_doc)\n",
    "    topic_dist = np.zeros(shape=(num_of_topics))\n",
    "    for topic, prob in topics:\n",
    "        topic_dist[topic] = prob\n",
    "    sims = jensen_shannon(topic_dist, doc_topic_matrix())\n",
    "    docs = list(sims.argsort()[:5])\n",
    "    return get_document(docs)\n",
    "\n",
    "\n",
    "def retrieval(doc):\n",
    "    doc = preprocessed_doc(doc)\n",
    "    doc = processed_doc(doc)\n",
    "    return get_similar_docs(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = '''Dear Sir/Madam,\n",
    "\n",
    " \n",
    "\n",
    "Atal Bihari Vajpayee-Indian Institute of Information Technology and Management Gwalior (ABV-IIITM Gwalior) is seeking admission for a PhD programme in the field of Engineering Technology (CS & IT, EC), Management and Applied Sciences (Mathematics & Physics).\n",
    "\n",
    "Please find the attached programme brochure with specific programme details. We would like to request you to kindly circulate the brochure in your Institute so that the information may reach to the prospective research scholars.  \n",
    "\n",
    "Sincerely yours,\n",
    "\n",
    "Pankaj Gupta\n",
    "\n",
    "Joint Registrar (Academics)\n",
    "\n",
    " \n",
    "\n",
    "About the Institute:\n",
    "\n",
    "Atal Bihari Vajpayee-Indian Institute of Information Technology and Management Gwalior (ABV-IIITM Gwalior), is an apex Information Technology (IT) and Management Institute, established by the Government of India. ABV-IIITM Gwalior has been declared an Institute of National importance. The Institute strives to become a world-class Institution which endeavors to carve young minds through teaching and research and develop them as tomorrow's leaders. The Institute's mandate is to create Information Technology enabled Management solutions for nation building. The Institute offers various programmes at UG/PG and Doctoral level\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Arduino workshop\n",
      "Text: Hi everyone,\n",
      "Those who are going to stay for Arduino workshop as Mentees, put your name\n",
      "and roll number in the google form.\n",
      "\n",
      "The names in the list will be finalized for group making and will be used\n",
      "for the certificates.\n",
      "\n",
      "\n",
      "https://goo.gl/forms/pLBCU9fWX6ZNJmMU2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "similar_docs = retrieval(doc)\n",
    "print(similar_docs[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
