{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import re\n",
    "import gensim\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pyLDAvis.gensim\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "from gensim.test.utils import datapath\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_lines = [\"It's not an attachment -- it's stored online. To open this item, just clickthe link above.\",\n",
    "                \"I've shared an item with you:\"]\n",
    "\n",
    "\n",
    "def tokenizing(doc):\n",
    "    \"\"\"\n",
    "    Convert a document into a list of lowercase tokens, ignoring tokens that are too short or too long.\n",
    "    \"\"\"\n",
    "    return gensim.utils.simple_preprocess(str(doc), deacc=True, min_len=4, max_len=15)\n",
    "\n",
    "\n",
    "def preprocessed_doc(doc):\n",
    "    \"\"\"\n",
    "    Preprocessing a single document\n",
    "    \"\"\"\n",
    "    for line in remove_lines:\n",
    "        doc = doc.replace(line, '')\n",
    "    \n",
    "    doc = re.sub(r'https?\\S*\\s?', '', doc)\n",
    "    doc = re.sub(r'\\s\\S*\\.ac\\.in\\S*\\s?', ' ', doc)\n",
    "    doc = re.sub(r'\\s\\S*\\.edu\\S*\\s?', ' ', doc)\n",
    "    doc = re.sub(r'\\s\\S*\\.com\\S*\\s?', ' ', doc)\n",
    "    doc = re.sub(r'www\\.\\S*\\s?', '', doc)\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc)\n",
    "    doc = re.sub(r'\\s+', ' ', doc)    \n",
    "    doc = tokenizing(doc)\n",
    "    return doc\n",
    "    \n",
    "    \n",
    "def preprocessed_data(data):\n",
    "    \"\"\"\n",
    "    Preprocessing the entire data (list of documents)\n",
    "    \"\"\"\n",
    "    return [preprocessed_doc(doc) for doc in data]\n",
    "\n",
    "\n",
    "df = pandas.read_json('data/processed_iitdh_broadcast-3.json')\n",
    "data = df.content.values.tolist()\n",
    "tokenized_data = preprocessed_data(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_ = ['dharwad', 'regard', 'iit', 'please', 'student',\n",
    "              'institute', 'write', 'thank', 'form', 'technology',\n",
    "              'would', 'year', 'time', 'follow', 'email', 'room', 'fill', 'date',\n",
    "              'also', 'indian', 'engineering', 'give', 'get', 'day', 'work', 'india',\n",
    "              'detail', 'mail', 'interested', 'use', 'request', 'guy', 'may',\n",
    "              'not', 'link', 'like', 'take', 'make', 'still', 'since', 'keep',\n",
    "              'secretary', 'come', 'one', 'professor', 'today', 'good',\n",
    "              'find', 'tomorrow', 'first', 'send', 'system', 'start',\n",
    "              'information', 'member', 'prof', 'part', 'regretted' 'registrar'\n",
    "              'new', 'venue', 'attend', 'kindly', 'aug', '-PRON-',\n",
    "              'image', 'provide', 'well', 'visit', 'do', 'inconvenience'\n",
    "              'assistant_professor', 'want', 'contact', 'go', 'meet',\n",
    "              'invite', 'name', 'need', 'opportunity', 'attach',\n",
    "              'everyone', 'google', 'receive', 'conduct', 'great', 'note', 'affair',\n",
    "              'available', 'number', 'august', 'many', 'college', 'share',\n",
    "              'help', 'programme', 'walmi_campus', 'belur_industrial',\n",
    "              'know', 'hold', 'participation', 'march', 'group', 'walmi'\n",
    "              'rule', 'see', 'learn', 'hello_everyone', 'indore', 'campus'\n",
    "              'challenge', 'phd', 'present', 'people', 'saturday', 'open', 'hope',\n",
    "              'april', 'jan', 'online', 'issue', 'require', 'play', 'thing',\n",
    "              'back', 'faculty', 'dean', 'summer', 'join', 'next', 'inform',\n",
    "              'base', 'topic', 'hello', 'session', 'idea', 'model', 'participant',\n",
    "              'post', 'last', 'change', 'round', 'karnataka_india', 'title', 'list',\n",
    "              'two', 'bring', 'access', 'app', 'include', 'click', 'sincerely', 'learn'\n",
    "              'view', 'hour', 'close', 'question', 'create', 'third', 'front', 'arpit', 'agrawal', 'btech',\n",
    "              'high', 'week', 'gentle_reminder', 'september', 'service', 'area', 'morning',\n",
    "              'timing', 'inter', 'from', 'subject', 'text', 'fwd', 'forwarded', 'message', 'dear', 'f',\n",
    "              'iitdh', 'iitg', 'deputation', 'three', 'become', 'karnataka', 'select', 'wbw_prasanna',\n",
    "              'unsubscribe', 'pron', 'hey', 'cse', 'january', 'february', 'march', 'april', 'june', 'july',\n",
    "              'august', 'september', 'october', 'november', 'december', 'larsen', 'tourbo', 'tourbo_chair',\n",
    "              'bombay', 'lot', 'sit', 'try', 'ive_invite', 'till', 'every', 'never', 'near', 'welfare', 'techf',\n",
    "              'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'secy', 'could',\n",
    "              'student', 'department', 'first', 'second', 'four', 'feel', 'reminder', 'gentle',  'sure',\n",
    "              'sign', 'photo', 'welcome', 'assistant', 'already', 'sorry', 'small', 'attachment', 'ever']\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(stopwords_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(tokenized_data, min_count=5, threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "\n",
    "def remove_stopwords(doc):\n",
    "    \"\"\"\n",
    "    Removes stopwords from a document\n",
    "    \"\"\"\n",
    "    return [word for word in doc\n",
    "            if word not in stop_words]\n",
    "\n",
    "\n",
    "def make_bigrams(doc):\n",
    "    \"\"\"\n",
    "    Make bigrams of a document\n",
    "    \"\"\"\n",
    "    return bigram_mod[doc]\n",
    "\n",
    "\n",
    "def lemmatization(doc):\n",
    "    \"\"\"\n",
    "    Lemmatizes a document\n",
    "    \"\"\"\n",
    "    doc = nlp(\" \".join(doc))\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "\n",
    "def processed_doc(doc):\n",
    "    \"\"\"\n",
    "    Processing a document\n",
    "    \"\"\"\n",
    "    doc = remove_stopwords(doc)\n",
    "    doc = make_bigrams(doc)\n",
    "    doc = lemmatization(doc)\n",
    "    doc = remove_stopwords(doc)\n",
    "    return doc\n",
    "\n",
    "\n",
    "def processed_data(data):\n",
    "    \"\"\"\n",
    "    Return lemmatized data\n",
    "    \"\"\"\n",
    "    return [processed_doc(doc) for doc in data]\n",
    "\n",
    "lemmatized_data = processed_data(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_topics = 10\n",
    "lda_model = gensim.models.LdaModel.load(datapath('iitdh_bow_10_0.01_0.61'))\n",
    "id2word = gensim.corpora.Dictionary(lemmatized_data)\n",
    "corpus = corpus = [id2word.doc2bow(text) for text in lemmatized_data]\n",
    "\n",
    "tfidf = gensim.models.TfidfModel(corpus)\n",
    "corpus = tfidf[corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics():\n",
    "    \"\"\"\n",
    "    Prints top 30 words from each topic\n",
    "    \"\"\"\n",
    "    pprint(lda_model.show_topics(num_of_topics, 30))\n",
    "\n",
    "\n",
    "def get_doc_topic_distribution(index):\n",
    "    \"\"\"\n",
    "    Get the topic distribution for a given document\n",
    "    \"\"\"\n",
    "    doc = corpus[index]\n",
    "    return lda_model.get_document_topics(doc)\n",
    "\n",
    "\n",
    "def get_topic_term():\n",
    "    \"\"\"\n",
    "    Get topic-term matrix\n",
    "    \"\"\"\n",
    "    topic_term_matrix = lda_model.get_topics()\n",
    "    print(len(topic_term_matrix), 'x', len(topic_term_matrix[0]))\n",
    "    return topic_term_matrix\n",
    "\n",
    "\n",
    "def get_term_topics(word_id):\n",
    "    \"\"\"\n",
    "    Get the most relevant topics to the given word\n",
    "    \"\"\"\n",
    "    print('Word is: ', id2word[word_id])\n",
    "    relevant_topics = lda_model.get_term_topics(word_id)\n",
    "    relevant_topic_ids = [topic_id[0] for topic_id in relevant_topics]\n",
    "    for topic_id in relevant_topic_ids:\n",
    "        print(show_topic(topic_id))\n",
    "    return relevant_topics\n",
    "\n",
    "\n",
    "def unseen_doc_topic_distribution(new_doc: list):\n",
    "    \"\"\"\n",
    "        Get topic distribution of a unseen document\n",
    "    \"\"\"\n",
    "    new_doc = [id2word.doc2bow(new_doc)]\n",
    "    topics = lda_model[new_doc]\n",
    "    return sorted(topics[0][0], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "def doc_topic_matrix():\n",
    "    \"\"\"\n",
    "    Get document topic matrix\n",
    "    \"\"\"\n",
    "    matrix = np.zeros(shape=(len(corpus), num_of_topics))\n",
    "    for i in range(len(corpus)):\n",
    "        topic_dist = get_doc_topic_distribution(i)\n",
    "        for topic, prob in topic_dist:\n",
    "            matrix[i][topic] = prob\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def jensen_shannon(query, matrix):\n",
    "    \"\"\"\n",
    "    This function implements a Jensen-Shannon similarity\n",
    "    between the input query (an LDA topic distribution for a document)\n",
    "    and the entire corpus of topic distributions.\n",
    "    It returns an array of length M where M is the number of documents in the corpus\n",
    "    \"\"\"\n",
    "    p = query[None, :].T\n",
    "    q = matrix.T\n",
    "    m = 0.5 * (p + q)\n",
    "    # entropy calculated KL divergence\n",
    "    return np.sqrt(0.5 * (entropy(p, m) + entropy(q, m)))\n",
    "\n",
    "\n",
    "def get_document(index):\n",
    "    \"\"\"\n",
    "    Index is list or num\n",
    "    \"\"\"\n",
    "    if isinstance(index, list):\n",
    "        docs = []\n",
    "        for i in index:\n",
    "            docs.append(data[i])\n",
    "        return docs\n",
    "    return data[index]\n",
    "\n",
    "\n",
    "def get_similar_docs(new_doc: list):\n",
    "    \"\"\"\n",
    "    This function implements the Jensen-Shannon distance above\n",
    "    and retruns the top k indices of the smallest jensen shannon distances\n",
    "    \"\"\"\n",
    "    topics = unseen_doc_topic_distribution(new_doc)\n",
    "    topic_dist = np.zeros(shape=(num_of_topics))\n",
    "    for topic, prob in topics:\n",
    "        topic_dist[topic] = prob\n",
    "    sims = jensen_shannon(topic_dist, doc_topic_matrix())\n",
    "    docs = list(sims.argsort()[:10])\n",
    "    return get_document(docs)\n",
    "\n",
    "\n",
    "def retrieval(doc):\n",
    "    doc = preprocessed_doc(doc)\n",
    "    doc = processed_doc(doc)\n",
    "    return get_similar_docs(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = '''\n",
    "Dear all,\n",
    "   Prof Krishna Maddaly from Ashoka University, Delhi will be speaking in our seminar.\n",
    "The talk is for general audience and all are invited. The details of  the talk is as follows:\n",
    "\n",
    "Title : Random Matrices in Quantum Mechanics\n",
    "Speaker: Prof Krishna Maddaly\n",
    "Date & Time: 8-11-2019 (Friday), 4:10 pm.\n",
    "Venue:  Room No - 023.\n",
    "\n",
    "Abstract: In this talk we will describe how random matrices play a role in describing some Quantum Mechanical systems and explain some problems connected with such matrices.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Re: A talk by Professor Van Maldeghem: 16 Jan, 2:30 pm, Room 023\n",
      "Text: A gentle reminder about today's talk.\n",
      "Amlan\n",
      "\n",
      "On Mon, Jan 14, 2019 at 12:42 PM Amlan Barua <abarua@iitdh.ac.in> wrote:\n",
      "\n",
      "> Dear All,\n",
      "> The talk by Professor Van Maldeghem is scheduled for 16th Jan at 2:30 pm.\n",
      "> You all are cordially invited in room 023 for the talk. Please find the\n",
      "> poster attached to know more about the talk.\n",
      "> Amlan\n",
      ">\n",
      "\n"
     ]
    }
   ],
   "source": [
    "similar_docs = retrieval(doc)\n",
    "print(similar_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Re: Talk by Dr. S. Vanka at 1400 in Rm no. 119\n",
      "Text: Dear colleagues and students,\n",
      "\n",
      "A gentle reminder for the talk today. The venue will be room number 119.\n",
      "\n",
      "Regards,\n",
      "Naveen\n",
      "\n",
      "On Mon, 8 Oct, 2018, 17:39 Naveen Kadayinti, <naveen@iitdh.ac.in> wrote:\n",
      "\n",
      ">\n",
      "> Dear Colleagues and Students\n",
      ">\n",
      "> We have a visitor, Dr. Sundaram Vanka, to the institute on 11th October.\n",
      "> He will be giving a talk on 11th October from 1400 to 1500 Hrs. The details\n",
      "> of the talk are as follows.\n",
      ">\n",
      "> Title : Interference Management in Emerging Wireless Networks and Systems\n",
      ">\n",
      "> Abstract: An important source of complexity in emerging wireless networks\n",
      "> is the increasingly diverse set of wireless devices and their applications.\n",
      "> Interference management is a key challenge in the design and operation of\n",
      "> such networks, and forms the overall theme that connects the speaker’s work\n",
      "> in diverse settings. This talk provides a brief overview of some aspects of\n",
      "> this work and is organized as follows. Part one situates his work in the\n",
      "> overall context of interference management and provides a bird’s eye view\n",
      "> of his work both in academia and in industry. Parts two and three will\n",
      "> discuss selected topics from his work in industry and as an academic\n",
      "> researcher, respectively. The talk concludes with some thoughts on the\n",
      "> challenges posed by the increased device heterogeneity expected in future\n",
      "> wireless networks.\n",
      ">\n",
      "> The talk will touch upon the speaker’s contributions to three challenging\n",
      "> problems in wireless LAN (WLAN, commonly called “Wi-Fi”) system design that\n",
      "> form some of the core technology in flagship wireless LAN SoC products. The\n",
      "> first two topics pertain to some of his work at  Broadcom on the general\n",
      "> problem of wireless coexistence, where devices with very different\n",
      "> communication requirements and constraints share a common spectrum. The\n",
      "> roles of interference avoidance, mitigation and adaptation are discussed in\n",
      "> the context of specific examples. It is pointed out how a holistic\n",
      "> perspective on the system-level impact of interference is necessary for\n",
      "> significantly improved coexistence. The third topic is selected to present\n",
      "> some of his work at Redpine Signals, Inc., that concerns the design of\n",
      "> low-complexity soft-output MIMO demodulators. It is shown how reasonably\n",
      "> accurate Likelihood-Ratios (LLRs) can be obtained without the need for\n",
      "> matrix inversion when the number of spatial streams is small, a scenario of\n",
      "> practical interest.\n",
      ">\n",
      "> As part of his doctoral research, the speaker has worked on multiple\n",
      "> theoretical and experimental topics. One representative topic from each\n",
      "> methodology will be presented here. The experimental work will discuss the\n",
      "> development of the first known prototype of a superposition coded wireless\n",
      "> system that was implemented on a software radio platform. In addition to\n",
      "> systematically developing experimentally robust analogs and methodologies\n",
      "> to theoretical constructs (such as the information-theoretic rate region),\n",
      "> this line of work also investigates some approaches that can leverage\n",
      "> superposition coding in practice. The theoretical topic will discuss his\n",
      "> work on deriving asymptotic scaling laws for the convergence behavior for a\n",
      "> class of distributed signal processing algorithms called average consensus\n",
      "> algorithms when message exchange between network nodes occurs over an\n",
      "> interference-limited wireless network. By combining the mathematical tools\n",
      "> from Markov chain theory, geometric random graph theory and information\n",
      "> theory, this line of work was the first to establish convergence results on\n",
      "> this algorithm in the presence of interference. In particular, it\n",
      "> challenges the long-held view that increased communication range always\n",
      "> improves the rate of convergence to consensus.\n",
      ">\n",
      "> The talk ends with some thoughts on the emergence of device heterogeneity,\n",
      "> and some challenges this poses in the context of the emerging Internet of\n",
      "> Things (IoT).\n",
      ">\n",
      "> Speaker bio: Sundaram Vanka received the B. Tech. and M. Tech. degrees\n",
      "> from the Indian Institute of Technology, Madras in 2003, and the Ph. D.\n",
      "> degree from the University of Notre Dame, Indiana, United States of America\n",
      "> in 2012, all in Electrical Engineering. Since 2012 he has been with the\n",
      "> wireless LAN (WLAN) systems architecture group at Broadcom Inc., a\n",
      "> market-leading designer, developer and supplier of WLAN chips on diverse\n",
      "> platforms, where he is currently Principal R&D engineer in the systems\n",
      "> architecture group. He leads systems R&D efforts into new system\n",
      "> architectures and protocols for interference management and coexistences\n",
      "> protocols, and defines the technology roadmap for these technologies. His\n",
      "> expertise and charter spans the mathematical modeling, design,\n",
      "> implementation, laboratory characterization and productization of\n",
      "> OFDM-based system design, interference-mitigation and coexistence protocols\n",
      "> for multiband/multichannel Wi-Fi and Wi-Fi/Bluetooth combo solutions. His\n",
      "> designs form a critical part of Broadcom’s flagship WLAN/Bluetooth combo\n",
      "> System-on-Chip (SoC) products that are shipped to leading consumer\n",
      "> electronics companies. His work has resulted in 15 US patents (7 granted, 8\n",
      "> pending).\n",
      ">\n",
      "> His research interests lie in wireless communications and networking,\n",
      "> spanning the mathematical modeling, simulation, design and prototyping of\n",
      "> wireless systems and networks. He believes the coming Internet of Things\n",
      "> opens up the possibility of re-evaluating some traditional assumptions\n",
      "> about wireless networks and offer new interesting topics for theoretical\n",
      "> and as well as experimental investigation.\n",
      ">\n",
      "> --\n",
      "> Regards,\n",
      "> Naveen\n",
      ">\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(similar_docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
