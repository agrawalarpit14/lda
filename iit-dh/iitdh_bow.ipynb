{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import re\n",
    "import gensim\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pyLDAvis.gensim\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "from gensim.test.utils import datapath\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_lines = [\"It's not an attachment -- it's stored online. To open this item, just clickthe link above.\",\n",
    "                \"I've shared an item with you:\"]\n",
    "\n",
    "\n",
    "def tokenizing(doc):\n",
    "    \"\"\"\n",
    "    Convert a document into a list of lowercase tokens, ignoring tokens that are too short or too long.\n",
    "    \"\"\"\n",
    "    return gensim.utils.simple_preprocess(str(doc), deacc=True, min_len=4, max_len=15)\n",
    "\n",
    "\n",
    "def preprocessed_doc(doc):\n",
    "    \"\"\"\n",
    "    Preprocessing a single document\n",
    "    \"\"\"\n",
    "    for line in remove_lines:\n",
    "        doc = doc.replace(line, '')\n",
    "    \n",
    "    doc = re.sub(r'https?\\S*\\s?', '', doc)\n",
    "    doc = re.sub(r'\\s\\S*\\.ac\\.in\\S*\\s?', ' ', doc)\n",
    "    doc = re.sub(r'\\s\\S*\\.edu\\S*\\s?', ' ', doc)\n",
    "    doc = re.sub(r'\\s\\S*\\.com\\S*\\s?', ' ', doc)\n",
    "    doc = re.sub(r'www\\.\\S*\\s?', '', doc)\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc)\n",
    "    doc = re.sub(r'\\s+', ' ', doc)    \n",
    "    doc = tokenizing(doc)\n",
    "    return doc\n",
    "    \n",
    "    \n",
    "def preprocessed_data(data):\n",
    "    \"\"\"\n",
    "    Preprocessing the entire data (list of documents)\n",
    "    \"\"\"\n",
    "    return [preprocessed_doc(doc) for doc in data]\n",
    "\n",
    "\n",
    "df = pandas.read_json('data/processed_iitdh_broadcast-3.json')\n",
    "data = df.content.values.tolist()\n",
    "tokenized_data = preprocessed_data(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_ = ['dharwad', 'regard', 'iit', 'please', 'student',\n",
    "              'institute', 'write', 'thank', 'form', 'technology',\n",
    "              'would', 'year', 'time', 'follow', 'email', 'room', 'fill', 'date',\n",
    "              'also', 'indian', 'engineering', 'give', 'get', 'day', 'work', 'india',\n",
    "              'detail', 'mail', 'interested', 'use', 'request', 'guy', 'may',\n",
    "              'not', 'link', 'like', 'take', 'make', 'still', 'since', 'keep',\n",
    "              'secretary', 'come', 'one', 'professor', 'today', 'good',\n",
    "              'find', 'tomorrow', 'first', 'send', 'system', 'start',\n",
    "              'information', 'member', 'prof', 'part', 'regretted' 'registrar'\n",
    "              'new', 'venue', 'attend', 'kindly', 'aug', '-PRON-',\n",
    "              'image', 'provide', 'well', 'visit', 'do', 'inconvenience'\n",
    "              'assistant_professor', 'want', 'contact', 'go', 'meet',\n",
    "              'invite', 'name', 'need', 'opportunity', 'attach',\n",
    "              'everyone', 'google', 'receive', 'conduct', 'great', 'note', 'affair',\n",
    "              'available', 'number', 'august', 'many', 'college', 'share',\n",
    "              'help', 'programme', 'walmi_campus', 'belur_industrial',\n",
    "              'know', 'hold', 'participation', 'march', 'group', 'walmi'\n",
    "              'rule', 'see', 'learn', 'hello_everyone', 'indore', 'campus'\n",
    "              'challenge', 'phd', 'present', 'people', 'saturday', 'open', 'hope',\n",
    "              'april', 'jan', 'online', 'issue', 'require', 'play', 'thing',\n",
    "              'back', 'faculty', 'dean', 'summer', 'join', 'next', 'inform',\n",
    "              'base', 'topic', 'hello', 'session', 'idea', 'model', 'participant',\n",
    "              'post', 'last', 'change', 'round', 'karnataka_india', 'title', 'list',\n",
    "              'two', 'bring', 'access', 'app', 'include', 'click', 'sincerely', 'learn'\n",
    "              'view', 'hour', 'close', 'question', 'create', 'third', 'front', 'arpit', 'agrawal', 'btech',\n",
    "              'high', 'week', 'gentle_reminder', 'september', 'service', 'area', 'morning',\n",
    "              'timing', 'inter', 'from', 'subject', 'text', 'fwd', 'forwarded', 'message', 'dear', 'f',\n",
    "              'iitdh', 'iitg', 'deputation', 'three', 'become', 'karnataka', 'select', 'wbw_prasanna',\n",
    "              'unsubscribe', 'pron', 'hey', 'cse', 'january', 'february', 'march', 'april', 'june', 'july',\n",
    "              'august', 'september', 'october', 'november', 'december', 'larsen', 'tourbo', 'tourbo_chair',\n",
    "              'bombay', 'lot', 'sit', 'try', 'ive_invite', 'till', 'every', 'never', 'near', 'welfare', 'techf',\n",
    "              'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'secy', 'could',\n",
    "              'student', 'department', 'first', 'second', 'four', 'feel', 'reminder', 'gentle',  'sure',\n",
    "              'sign', 'photo', 'welcome', 'assistant', 'already', 'sorry', 'small', 'attachment', 'ever']\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(stopwords_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(tokenized_data, min_count=5, threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "\n",
    "def remove_stopwords(doc):\n",
    "    \"\"\"\n",
    "    Removes stopwords from a document\n",
    "    \"\"\"\n",
    "    return [word for word in doc\n",
    "            if word not in stop_words]\n",
    "\n",
    "\n",
    "def make_bigrams(doc):\n",
    "    \"\"\"\n",
    "    Make bigrams of a document\n",
    "    \"\"\"\n",
    "    return bigram_mod[doc]\n",
    "\n",
    "\n",
    "def lemmatization(doc):\n",
    "    \"\"\"\n",
    "    Lemmatizes a document\n",
    "    \"\"\"\n",
    "    doc = nlp(\" \".join(doc))\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "\n",
    "def processed_doc(doc):\n",
    "    \"\"\"\n",
    "    Processing a document\n",
    "    \"\"\"\n",
    "    doc = remove_stopwords(doc)\n",
    "    doc = make_bigrams(doc)\n",
    "    doc = lemmatization(doc)\n",
    "    doc = remove_stopwords(doc)\n",
    "    return doc\n",
    "\n",
    "\n",
    "def processed_data(data):\n",
    "    \"\"\"\n",
    "    Return lemmatized data\n",
    "    \"\"\"\n",
    "    return [processed_doc(doc) for doc in data]\n",
    "\n",
    "\n",
    "lemmatized_data = processed_data(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_topics = 10\n",
    "lda_model = gensim.models.LdaModel.load(datapath('iitdh_bow_10_0.01_0.61'))\n",
    "id2word = lda_model.id2word\n",
    "corpus = corpus = [id2word.doc2bow(text) for text in lemmatized_data]\n",
    "\n",
    "# tfidf = gensim.models.TfidfModel(corpus)\n",
    "# corpus = tfidf[corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics():\n",
    "    \"\"\"\n",
    "    Prints top 30 words from each topic\n",
    "    \"\"\"\n",
    "    pprint(lda_model.show_topics(num_of_topics, 30))\n",
    "\n",
    "\n",
    "def get_doc_topic_distribution(index):\n",
    "    \"\"\"\n",
    "    Get the topic distribution for a given document\n",
    "    \"\"\"\n",
    "    doc = corpus[index]\n",
    "    return lda_model.get_document_topics(doc)\n",
    "\n",
    "\n",
    "def get_topic_term():\n",
    "    \"\"\"\n",
    "    Get topic-term matrix\n",
    "    \"\"\"\n",
    "    topic_term_matrix = lda_model.get_topics()\n",
    "    print(len(topic_term_matrix), 'x', len(topic_term_matrix[0]))\n",
    "    return topic_term_matrix\n",
    "\n",
    "\n",
    "def get_term_topics(word_id):\n",
    "    \"\"\"\n",
    "    Get the most relevant topics to the given word\n",
    "    \"\"\"\n",
    "    print('Word is: ', id2word[word_id])\n",
    "    relevant_topics = lda_model.get_term_topics(word_id)\n",
    "    relevant_topic_ids = [topic_id[0] for topic_id in relevant_topics]\n",
    "    for topic_id in relevant_topic_ids:\n",
    "        print(show_topic(topic_id))\n",
    "    return relevant_topics\n",
    "\n",
    "\n",
    "def unseen_doc_topic_distribution(new_doc: list):\n",
    "    \"\"\"\n",
    "    Get topic distribution of a unseen document\n",
    "    \"\"\"\n",
    "    new_doc = [id2word.doc2bow(new_doc)]\n",
    "    topics = lda_model[new_doc]\n",
    "    return sorted(topics[0][0], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "def doc_topic_matrix():\n",
    "    \"\"\"\n",
    "    Get document topic matrix\n",
    "    \"\"\"\n",
    "    matrix = np.zeros(shape=(len(corpus), num_of_topics))\n",
    "    for i in range(len(corpus)):\n",
    "        topic_dist = get_doc_topic_distribution(i)\n",
    "        for topic, prob in topic_dist:\n",
    "            matrix[i][topic] = prob\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def jensen_shannon(query, matrix):\n",
    "    \"\"\"\n",
    "    This function implements a Jensen-Shannon similarity\n",
    "    between the input query (an LDA topic distribution for a document)\n",
    "    and the entire corpus of topic distributions.\n",
    "    It returns an array of length M where M is the number of documents in the corpus\n",
    "    \"\"\"\n",
    "    p = query[None, :].T\n",
    "    q = matrix.T\n",
    "    m = 0.5 * (p + q)\n",
    "    # entropy calculated KL divergence\n",
    "    return np.sqrt(0.5 * (entropy(p, m) + entropy(q, m)))\n",
    "\n",
    "\n",
    "def get_document(index):\n",
    "    \"\"\"\n",
    "    Index is list or num\n",
    "    \"\"\"\n",
    "    if isinstance(index, list):\n",
    "        docs = []\n",
    "        for i in index:\n",
    "            docs.append(data[i])\n",
    "        return docs\n",
    "    return data[index]\n",
    "\n",
    "\n",
    "def get_similar_docs(new_doc: list):\n",
    "    \"\"\"\n",
    "    This function implements the Jensen-Shannon distance above\n",
    "    and retruns the top k indices of the smallest jensen shannon distances\n",
    "    \"\"\"\n",
    "    topics = unseen_doc_topic_distribution(new_doc)\n",
    "    print(topics)\n",
    "    topic_dist = np.zeros(shape=(num_of_topics))\n",
    "    for topic, prob in topics:\n",
    "        topic_dist[topic] = prob\n",
    "    sims = jensen_shannon(topic_dist, doc_topic_matrix())\n",
    "    docs = list(sims.argsort()[:10])\n",
    "    return get_document(docs)\n",
    "\n",
    "\n",
    "def retrieval(doc):\n",
    "    doc = preprocessed_doc(doc)\n",
    "    doc = processed_doc(doc)\n",
    "    return get_similar_docs(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = '''\n",
    "Dear all,\n",
    "   Prof Krishna Maddaly from Ashoka University, Delhi will be speaking in our seminar.\n",
    "The talk is for general audience and all are invited. The details of  the talk is as follows:\n",
    "\n",
    "Title : Random Matrices in Quantum Mechanics\n",
    "Speaker: Prof Krishna Maddaly\n",
    "Date & Time: 8-11-2019 (Friday), 4:10 pm.\n",
    "Venue:  Room No - 023.\n",
    "\n",
    "Abstract: In this talk we will describe how random matrices play a \n",
    "role in describing some Quantum Mechanical systems and explain some problems connected with such matrices.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 0.88956225), (9, 0.107588105)]\n",
      "Subject: Interaction Session with Vivek Pawar, CEO, DF and C M Patil, CEO, Sandbox StartUps\n",
      "Text: Dear All,\n",
      "The interaction session with Vivek Pawar, CEO, Deshpande Foundation and C M\n",
      "Patil, CEO, Sandbox StartUps is scheduled on Tuesday, 7th Aug 2018 as per\n",
      "the following:\n",
      "\n",
      "Venue: Board Room, IIT Dharwad\n",
      "\n",
      "Time: 5 to 6.30 pm.\n",
      "\n",
      "Tentative Agenda:\n",
      "5.00-5.20 pm: Presentation by Shri Vivek Pawar, CEO, DF about the\n",
      "activities of DF\n",
      "5.20-5.30 pm: Interaction with Vivek Pawar\n",
      "5.30-5.50 pm: Presentation by Shri C. M. Patil, CEO, Sandbox StartUps about\n",
      "the activities\n",
      "5.50-6.00 pm: Interaction with C M Patil\n",
      "6.00-6.30 pm: Meeting of CEOs with Deans Committee and Director\n",
      "\n",
      "wbw\n",
      "Prasanna.\n",
      "-----\n",
      "S. R. Mahadeva Prasanna\n",
      "Professor, Dept. of Electrical Engineering (on deputation from IITG)\n",
      "Dean (Faculty Welfare, Research & Development)\n",
      "Indian Institute of Technology Dharwad\n",
      "Dharwad - 580011\n",
      "dean.rnd@iitdh.ac.in\n",
      "9954008138, 8638535542\n",
      "836-2212840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "similar_docs = retrieval(doc)\n",
    "print(similar_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 0.88952917), (9, 0.107621126)]\n"
     ]
    }
   ],
   "source": [
    "new = preprocessed_doc(doc)\n",
    "new = processed_doc(new)\n",
    "print(unseen_doc_topic_distribution(new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
